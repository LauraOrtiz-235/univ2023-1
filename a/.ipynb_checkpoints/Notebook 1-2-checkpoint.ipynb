{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9e75e0d3292f425f9e178c7502b651f0",
    "deepnote_app_coordinates": {
     "h": 22,
     "w": 12,
     "x": 0,
     "y": 1
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<img src=\"https://s3.amazonaws.com/media-p.slid.es/uploads/1485763/images/9060062/Header.png\" alt=\"Header\" style=\"width: 800px;\"/>\n",
    "\n",
    "# Aprendizaje Automático de Máquina\n",
    "\n",
    "Yiby Karolina Morales Pinto\n",
    "\n",
    "```\n",
    "💡Integrantes💡\n",
    "\n",
    "✨Laura Sofía Ortiz Arcos\n",
    "✨David Santiago Flórez Alsina\n",
    "```    \n",
    "\n",
    "<hr> </hr>\n",
    "\n",
    "Abril 27, 2023\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "73701962355542cc835a14a30fd90981",
    "deepnote_app_coordinates": {
     "h": 28,
     "w": 12,
     "x": 0,
     "y": 24
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Antecedentes\n",
    "\n",
    "_**Opcional:**_ Nuestra motivación para trabajar este problema es utilizar unos datos que recolectamos aproximadamente hace un año en el laboratorio Bardeen en la primera edición del proyecto de emprendimiento ___Maticas___.\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./albahaca_maticas_v3_heavy.png\" alt=\"Header\" style=\"width: 400px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "***Maticas*** surge de ver cómo en *Colombia* se tratan de generar soluciones para ayudar al agricultor y permitirle ser más productivo, en un tiempo en el que se habla mucho de reformas agrarias y distribución de tierras, pero el debate no inicia oficialmente y menos el planteamiento de una solución y su implementación, mientras que a la vez las nuevas generaciones de personas que nacen en estas zonas rurales ya no quieren dedicarse a los campos ya que la ven como una actividad muy exaustiva y con poco retorno económico en comparación con lo que ofrece la ciudad. *Maticas* busca que a través de la tecnología se le pueda dar solución a parte del problema del agro que el estado aún no ha solucionado.\n",
    "\n",
    "\n",
    "El inicio y actual propósito ***Maticas*** es ayudar al agricultor a *esforzarse menos* en su trabajo, obteniendo *mejores resultados*, permitiendo a todos los consumidores el tener *más variedad de plantas, de mejor calidad y mejor precio* para alimentarnos. Nuestro objetivo no solo busca fortalecer el agro en el sector rural, sino también en el urbarno, a través de huertas urbanas automatizadas con hidroponía que se puedan ubicar en distintos espacios y empresas.\n",
    "\n",
    "En un futuro próximo buscamos incorporar inteligencia artificial para ayudar al agricultor a tomar acciones que protejan su cultivo, si por ejemplo pudieramos detectar que dentro de 48 horas va a haber una helada, se podrían tomar ciertas acciones que mitiguen los efectos sobre el cultivo, por ejemplo. Lo que buscamos con este proyecto es acercarnos un poco a esto.\n",
    "\n",
    "Hay trabajos que hacen labores similares de predicción muchos de ellos usan redes neuronales recurrentes, sin embargo nosotros queremos probar una aproximación desde las redes neuronales sencillas y la extracción de caracterísisticas temporales, de nuestras señales.\n",
    "\n",
    "links para hablar: \n",
    "\n",
    "- https://repositorio.uniandes.edu.co/bitstream/handle/1992/45458/u827540.pdf?sequence=1 \n",
    "\n",
    "- https://upcommons.upc.edu/bitstream/handle/2117/108387/memoria-tfm-federico-aguilar-calvo.pdf?sequence=1&isAllowed=y\n",
    "\n",
    "- http://somim.org.mx/memorias/memorias2020/articulos/A4_6.pdf\n",
    "\n",
    "- https://oa.upm.es/11206/1/MARIA_GUADALUPE_CORTINA_JANUCHS.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2ac47b9745b64b33b702f43c43ec9160",
    "deepnote_app_coordinates": {
     "h": 10,
     "w": 12,
     "x": 0,
     "y": 53
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Definición del problema\n",
    "\n",
    "Se describe claramente el problema que se resolverá. El problema debe estar bien definido, debe ser cuantificable y medible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "79fe9463d5a4469ca04b5ec932f8f562",
    "deepnote_app_coordinates": {
     "h": 9,
     "w": 12,
     "x": 0,
     "y": 64
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "_Nos planteamos solucionar la problemática de predecir humedad y temperatura ambiental usando redes neuronales y tensorflow, lo haremos con predicciones para 10 min, 15 min, 30 min y 60 min en el futuro_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "85c8a89dd8e1425a927a9f5c2a4f75d2",
    "deepnote_app_coordinates": {
     "h": 11,
     "w": 12,
     "x": 0,
     "y": 74
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Descripción de la solución\n",
    "\n",
    "Descripción de cómo se abordará el problema. La estrategia planteada es aplicable al proyecto y apropiada para los datos dados. Adicionalmente, la solución es cuantificable y medible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "47ff617a6a58419ebd0152a2bcc82ce0",
    "deepnote_app_coordinates": {
     "h": 8,
     "w": 12,
     "x": 0,
     "y": 86
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "_Construir una red neuronal predictiva para los periodos de tiempo seleccionados, probaremos distintas arquitecturas y funciones de activación, para encontrar la mejor red de todas._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "129e577f9e3b4513836be5df0f343c40",
    "deepnote_app_coordinates": {
     "h": 10,
     "w": 12,
     "x": 0,
     "y": 95
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Datos\n",
    "\n",
    "El dataset a usar en el proyecto debe ser descrito detalladamente. Información acerca de dónde se obtuvo el dataset y sus características, debe ser incluída. Así como toda referencia utilizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e12de02cde9542adb32812d53f933b53",
    "deepnote_app_coordinates": {
     "h": 12,
     "w": 12,
     "x": 0,
     "y": 106
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Nuestro dataset se compone de las siguientes mediciones:\n",
    "- Luz (lux). \n",
    "- Humedad relativa (%). \n",
    "- Tempertura ambiental (°C). \n",
    "- Presión atmosférica (mmHg).\n",
    "\n",
    "Cada medición tiene un timestamp asociado que nos indica el instante en el que se midió."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bd797aa78e45449aa8d649210917f051",
    "deepnote_app_coordinates": {
     "h": 11,
     "w": 12,
     "x": 0,
     "y": 119
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Modelo de referencia\n",
    "\n",
    "Se debe hacer un experimento preliminar con un primer algoritmo, puede ser simple. Esto con el fin de obtener una primera aproximación al problema, la cual se deberá mejorar durante el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "65fed2f6a8fc4486905fb3974eda9bd5",
    "deepnote_app_coordinates": {
     "h": 18,
     "w": 12,
     "x": 0,
     "y": 131
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9558,
    "execution_start": 1683840309384,
    "source_hash": "90981e97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 18:20:09.574475: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-13 18:20:09.735368: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-13 18:20:09.736540: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-13 18:20:11.153039: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import datetime          as dt\n",
    "import pandas            as pd\n",
    "import tensorflow        as tf\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn                 import preprocessing\n",
    "from tensorflow              import keras\n",
    "from datetime                import timedelta\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "592560f4bab944c7b69231cea8d02c50",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Preprocesamiento de los datos\n",
    "\n",
    "$$(1)$$ Indexamos cada dato con su timestamp en lugar de un índice numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "fcd965cbbe334232adf683a9169052e4",
    "deepnote_app_coordinates": {
     "h": 15,
     "w": 12,
     "x": 0,
     "y": 150
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1683840318938,
    "source_hash": "d4fbddc7"
   },
   "outputs": [],
   "source": [
    "def separate_timestamp(df):\n",
    "    \"\"\"\n",
    "    Converts the 'time' column in the input dataframe to a pandas datetime format.\n",
    "    \n",
    "    INPUT:\n",
    "        - df: A pandas DataFrame with a 'time' column\n",
    "    \n",
    "    OUTPUT:\n",
    "        - pd.DataFrame: A copy of the input dataframe with the 'time' column converted to pandas datetime format\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the 'time' column to a pandas datetime format\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # Return the modified dataframe\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5a73402a9daa4062a4ee4f0d5229b1df",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "$$(2)$$ Extraemos algunas características en intervalos de tiempo seleccionados por nosotros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "063d0cd639294844886ecf3fa8962def",
    "deepnote_app_coordinates": {
     "h": 28,
     "w": 12,
     "x": 0,
     "y": 166
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 26,
    "execution_start": 1683840318961,
    "source_hash": "bbdd5624"
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_rolling_average(df: pd.DataFrame, N: list, var: str):\n",
    "    \"\"\"\n",
    "    Calculates rolling statistics of a specified variable for a given window size and adds them as new columns\n",
    "    to the input dataframe.\n",
    "    \n",
    "    Args:\n",
    "    - df (pandas DataFrame): input dataframe containing a column named 'time' and the variable of interest \n",
    "                             specified in the 'var' argument.\n",
    "    - N (list of integers):  list of window sizes (in minutes) for which to calculate rolling statistics.\n",
    "    - var (string):          name of the variable of interest for which to calculate rolling statistics.\n",
    "    \n",
    "    Returns:\n",
    "    - pandas DataFrame:      modified dataframe with new columns added for the rolling statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the 'time' column as the dataframe index\n",
    "    df.set_index('time', inplace=True)\n",
    "\n",
    "    # Calculate the rolling statistics of the 'var' column for each window size in the list N\n",
    "    for n in N:\n",
    "        rolling_average   = df[var].rolling(f'{n}min').mean()\n",
    "        rolling_variance  = df[var].rolling(f'{n}min').var()\n",
    "        rolling_minimum   = df[var].rolling(f'{n}min').min()\n",
    "        rolling_maximum   = df[var].rolling(f'{n}min').max()\n",
    "\n",
    "        # Add the rolling statistics as new columns to the dataframe, filling NaN values with the next valid value\n",
    "        df[f'avg_{n}min']  = rolling_average.bfill()\n",
    "        df[f'var_{n}min']  = rolling_variance.bfill()\n",
    "        df[f'max_{n}min']  = rolling_maximum.bfill()\n",
    "        df[f'min_{n}min']  = rolling_minimum.bfill()\n",
    "\n",
    "    # Return the modified dataframe\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "d7abdfc3e6944f4db7397073f74eaa27",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 195
    },
    "deepnote_cell_type": "code",
    "deepnote_table_loading": false,
    "deepnote_table_state": {
     "filters": [],
     "pageIndex": 0,
     "pageSize": 10,
     "sortBy": []
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 54,
    "execution_start": 1683840318990,
    "source_hash": "c356a434"
   },
   "outputs": [],
   "source": [
    "df = separate_timestamp( pd.read_csv('hum.csv') )\n",
    "features = calculate_rolling_average(df, [10, 15, 30, 60], 'hum_level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ec0ff9fd310d47fba3c283611df10938",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "$$(3)$$ Escalamos nuestros datos para probar si nos dan mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "67054db6f99145d58cf4091964837ed5",
    "deepnote_app_coordinates": {
     "h": 25,
     "w": 12,
     "x": 0,
     "y": 200
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1683840319057,
    "source_hash": "6145b3b4"
   },
   "outputs": [],
   "source": [
    "def data_scaling(data: pd.DataFrame,\n",
    "                 scaler: sklearn.preprocessing._data,\n",
    "                 scaling_columns     = [], \n",
    "                 not_scaling_columns = [], \n",
    "                 dropping = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Scales the specified columns in the input DataFrame using the given scaler object.\n",
    "    \n",
    "    INPUT:\n",
    "        - data:                Pandas DataFrame to be scaled\n",
    "        - scaler:              Scaler object from sklearn.preprocessing\n",
    "        - scaling_columns:     List of column names to scale\n",
    "        - not_scaling_columns: List of column names to not scale\n",
    "        \n",
    "    OUTPUT: \n",
    "        - pd.DataFrame:        A scaled copy of the input data with the specified columns scaled\n",
    "    \"\"\"\n",
    "\n",
    "    # if the user provided all the columns he wishes to scale \n",
    "    if scaling_columns:\n",
    "        data[scaling_columns] = scaler.fit_transform(data[scaling_columns].to_numpy())\n",
    "\n",
    "    # if user provided only the columns he doesn't want to scale\n",
    "    elif not_scaling_columns:\n",
    "        used_cols = data.columns.difference(not_scaling_columns)\n",
    "        data.loc[:, used_cols] = scaler.fit_transform(data.loc[:, used_cols].to_numpy())\n",
    "\n",
    "    else:\n",
    "        data.iloc[:] = scaler.fit_transform(data.iloc[:].to_numpy())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "da5991782cbc4eb5a64ade448ee03435",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "$$(4)$$ Construimos la matrix $$Y$$ que es una lista de salidas que buscamos obtener con nuestra predicción, donde cada fila tiene 4 columnas y cada una es el valor en 10, 15, 30 y 60 minutos en el futuro para la humedad relativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "66904c16fe59450da6bfbceae3dcfa0b",
    "deepnote_app_coordinates": {
     "h": 34,
     "w": 12,
     "x": 0,
     "y": 226
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 45,
    "execution_start": 1683840319066,
    "source_hash": "2eee4e5"
   },
   "outputs": [],
   "source": [
    "def find_future_measurement(data: pd.DataFrame,\n",
    "                            curr_measurement: pd.core.series.Series,\n",
    "                            prediction_var: str,\n",
    "                            N: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Takes a complete dataframe (data) and a row of this dataframe (curr_measurement), and finds the future measurements \n",
    "    for a given list of time intervals (N).\n",
    "    \n",
    "    INPUT:\n",
    "        - data: A pandas DataFrame containing the measurement data\n",
    "        - curr_measurement: A pandas Series containing a single row (measurement) of the DataFrame (data)\n",
    "        - prediction_var: A string representing the name of the column containing the measurement to predict\n",
    "        - N: A list of time intervals (in minutes) for which future measurements are to be predicted\n",
    "        \n",
    "    OUTPUT:\n",
    "        - A tuple containing the future measurement values for each of the time intervals in N\n",
    "    \n",
    "    Example:\n",
    "    If N = [30, 60], then the function will return a tuple containing the predicted measurements 30 and 60 minutes \n",
    "    into the future from the current measurement in curr_measurement.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Take current measurement index (datetime)\n",
    "    curr_idx = curr_measurement.name\n",
    "    \n",
    "    # Go n units into the future for n in N\n",
    "    nearest_measurements = []\n",
    "    \n",
    "    for n in N:\n",
    "        # Calculate the datetime n minutes into the future\n",
    "        future_time = curr_idx + pd.Timedelta(minutes=n)\n",
    "        \n",
    "        # Calculate the time difference between each index in data and the future_time\n",
    "        time_diff = abs(data.index - future_time)\n",
    "        \n",
    "        # Find the index of the row with the smallest time difference (i.e., closest measurement)\n",
    "        nearest_idx = time_diff.argmin()\n",
    "        \n",
    "        # Get the row with the closest measurement and add the predicted measurement to the list\n",
    "        nearest_time = data.iloc[nearest_idx]\n",
    "        nearest_measurements.append(nearest_time[prediction_var])\n",
    "    \n",
    "    return tuple(nearest_measurements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a902a59212cf4e96bb4a29655d481cbc",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "$$(5)$$ Integramos todo lo anterior en nuestra función build dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "3c99e46aec1843ba864fbbe4ee598aa1",
    "deepnote_app_coordinates": {
     "h": 31,
     "w": 12,
     "x": 0,
     "y": 261
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1683840319110,
    "source_hash": "57a7fcff"
   },
   "outputs": [],
   "source": [
    "def build_dataset(data: pd.DataFrame, N: list, prediction_var: str, scaler=None):\n",
    "    \"\"\"\n",
    "    Builds a dataset for time series prediction by creating X and Y matrices from the input data.\n",
    "\n",
    "    INPUT:\n",
    "        - data:           Pandas DataFrame containing the input data\n",
    "        - N:              List of integers representing the number of minutes into the future to predict for each sample\n",
    "        - prediction_var: String representing the name of the column to predict\n",
    "        - scaler:         Scaler object from sklearn.preprocessing to scale the input data\n",
    "\n",
    "    OUTPUT:\n",
    "        - Tuple of two numpy arrays: X matrix and Y matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the last hour of measurement\n",
    "    last_hour = data.tail(1).index[0] - pd.Timedelta(minutes=max(N))\n",
    "\n",
    "    # Drop the last hour of data\n",
    "    X = data.truncate(after=pd.Timestamp(last_hour))\n",
    "\n",
    "    # Build the Y matrix (the future measurements)\n",
    "    output_dims = (len(X), len(N))\n",
    "    Y = np.zeros(output_dims)\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        current = X.iloc[i].name\n",
    "        Y[i, :] = find_future_measurement(data = X.truncate(after=current),\n",
    "                                          N    = N,\n",
    "                                          curr_measurement = X.iloc[i],\n",
    "                                          prediction_var   = prediction_var)\n",
    "\n",
    "    # Scale the data if requested\n",
    "    if scaler is not None:\n",
    "        X = data_scaling(data=X, scaler=scaler, not_scaling_columns=[prediction_var])\n",
    "\n",
    "    # Use all the columns but the column we want to predict\n",
    "    cols = X.columns.difference([prediction_var])\n",
    "\n",
    "    return X[cols].to_numpy(), Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b66db4715c1f49e7842bf2b160aa9beb",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "$$(6)$$ Hacemos el train/test split para entrenar nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "1311f993d0564b5aafd9a76d818b50ad",
    "deepnote_app_coordinates": {
     "h": 9,
     "w": 12,
     "x": 0,
     "y": 293
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 39942,
    "execution_start": 1683840319151,
    "source_hash": "a854a205"
   },
   "outputs": [],
   "source": [
    "X, Y = build_dataset( data = features,\n",
    "                      N = [10, 15, 30, 60],\n",
    "                      prediction_var = 'hum_level')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    train_size   = 0.8,\n",
    "                                                    #shuffle = False,)\n",
    "                                                    random_state = 69,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9c51e3b3cdce4bcb866a94732429caf5",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Creación del modelo\n",
    "\n",
    "$$(1)$$ Modelo secuencial con función de activación ReLu y métrica de pérdida 'mse'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "179ffeb824ec424d916cf278ef92e7cf",
    "deepnote_app_coordinates": {
     "h": 8,
     "w": 12,
     "x": 0,
     "y": 303
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 20,
    "execution_start": 1683840359133,
    "source_hash": "3ffabe0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 18:20:41.832705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-13 18:20:41.834950: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    }
   ],
   "source": [
    "model =  keras.Sequential([ keras.layers.Dense(units=16, activation='relu'),\n",
    "                            keras.layers.Dense(units=32, activation='relu'),\n",
    "                            keras.layers.Dense(units=48, activation='relu'),\n",
    "                            keras.layers.Dense(units=4, activation='relu')])\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.01),\n",
    "              loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "19fdab0f1e7541739e366261e41644e9",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "$$(2)$$ Configuración de hiperparámetros y entrenamiento del modelo, así como guardado de su historial para entender su evolución en el tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "66ca2918d95f49e4b886aef7b9b6f347",
    "deepnote_app_coordinates": {
     "h": 30,
     "w": 12,
     "x": 0,
     "y": 312
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 46752,
    "execution_start": 1683840434596,
    "source_hash": "f1283ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 2ms/step - loss: 590.8177 - val_loss: 591.3016\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 589.2266 - val_loss: 589.8150\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 592.0580 - val_loss: 591.0375\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 588.1240 - val_loss: 592.3433\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 592.9103 - val_loss: 589.9087\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 587.6174 - val_loss: 590.6229\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 588.9661 - val_loss: 590.4423\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 587.9001 - val_loss: 591.3406\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 588.4489 - val_loss: 589.8704\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 590.9172 - val_loss: 590.7881\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 590.5085 - val_loss: 590.1684\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 587.5443 - val_loss: 590.4315\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 589.0808 - val_loss: 589.7751\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 592.8438 - val_loss: 589.8054\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 589.9146 - val_loss: 591.0372\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 589.5291 - val_loss: 590.6905\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 587.0942 - val_loss: 590.2773\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 589.0364 - val_loss: 589.7619\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 590.7111 - val_loss: 590.2521\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 588.1368 - val_loss: 591.5848\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 591.3968 - val_loss: 589.9582\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 589.0824 - val_loss: 589.8944\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 588.4118 - val_loss: 590.1922\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 587.6038 - val_loss: 590.3840\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 589.5132 - val_loss: 589.9362\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 588.5275 - val_loss: 589.7683\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 590.2000 - val_loss: 590.6729\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 590.1591 - val_loss: 589.7549\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 589.9250 - val_loss: 590.9283\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 586.8530 - val_loss: 589.7631\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 590.4079 - val_loss: 589.7819\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 587.9715 - val_loss: 589.7631\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 589.7867 - val_loss: 590.2652\n",
      "Epoch 34/50\n",
      " 855/1000 [========================>.....] - ETA: 0s - loss: 589.1590"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs           = 50, \n",
    "                    batch_size       = 3,\n",
    "                    steps_per_epoch  = 1000,\n",
    "                    validation_split = 0.10,\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "daceeeaa16a5446d846a330e0e365104",
    "deepnote_app_coordinates": {
     "h": 10,
     "w": 12,
     "x": 0,
     "y": 343
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1683840407243,
    "source_hash": "a07100b8"
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.plot(history.history['loss'], 'g', label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], 'b', label='Validation Loss')\n",
    "    #plt.xlim([0, 100])\n",
    "    #plt.ylim([0, 300])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b62b5e5f5b8046c083267a03096a4e53",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Observación de resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "5bdce3ecc82a47c2a5a2d50b41993c64",
    "deepnote_app_coordinates": {
     "h": 13,
     "w": 12,
     "x": 0,
     "y": 354
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 758,
    "execution_start": 1683840407248,
    "source_hash": "39089663"
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "cf988d63e5e84f91bf96e404944f3baa",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 458,
    "execution_start": 1683840408078,
    "source_hash": "1febaa2d"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "57c8c7461b594b779fb0e3a38d912947",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 63,
    "execution_start": 1683840408587,
    "source_hash": "b9901948"
   },
   "outputs": [],
   "source": [
    "plt.scatter(list(range(len(y_test[:,0]))), abs(y_pred[:,0] - y_test[:,0]), s=1.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8c414b30974c43ab9e64fc693cf5e1b9",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 478,
    "execution_start": 1683840408902,
    "source_hash": "bf59e26d"
   },
   "outputs": [],
   "source": [
    "plt.scatter(list(range(len(y_test[:,0]))), y_test[:,3], s=1.3)\n",
    "plt.scatter(list(range(len(y_pred[:,0]))), y_pred[:,3], s=1.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, X_test, y_test, N=[10, 15, 30, 60]):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    mse_values = []\n",
    "    \n",
    "    for n in N:\n",
    "        mse = np.mean(np.abs(y_test[:,n] - y_pred[:,n]))\n",
    "        mse_values.append(mse)\n",
    "        plt.scatter(list(range(len(y_test[:,n]))), y_test[:,n], s=1.3, label=f\"True (N={n})\")\n",
    "        plt.scatter(list(range(len(y_pred[:,n]))), y_pred[:,n], s=1.3, label=f\"Predicted (N={n})\")\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.title(\"True vs. Predicted values\")\n",
    "    plt.show()\n",
    "    print(f\"MSE values: {mse_values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "dc606d8a91cc4b9485b354e45df8a19c",
    "deepnote_app_coordinates": {
     "h": 12,
     "w": 12,
     "x": 0,
     "y": 368
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Métricas\n",
    "\n",
    "Se propone al menos una métrica de evaluación para cuantificar el desempeño del modelo. Las métricas propuestas son apropiadas para el problema. Un resumen del flujo de trabajo para abordar el proyecto. Se comentan estrategias a utilizar, qué análisis de los datos se podría requerir o qué algoritmos se considerarán."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2f341124d0a74c2a8be9b438d075188c",
    "deepnote_app_coordinates": {
     "h": 52,
     "w": 12,
     "x": 0,
     "y": 381
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Estrategias y flujo de trabajo\n",
    "\n",
    "1. Preprocesamiento de datos, esto es: \n",
    "    - Extracción de características _(dada la naturaleza temporal de los datos, exploramos la extracción de características en intervalos de 10, 15, 30 y 60 minutos, para ayudar a nuestro modelo a hacer predicciones, sin recurrir a arquitecturas más complejas)_.\n",
    "    -  Escalamiento de datos (vamos a probar si es mejor con o sin este escalamiento).\n",
    "    - Combinación de las distintas bases de datos de mediciones de variables ambientales _(en la prueba de concepto presentada anteriormente se hizo uso únicamente de los datos de humedad para predecir los datos futuros de humedad, sin embargo queremos explorar si el incluir la influencia de otras variables ambientales nos ayuda a mejorar el resultado)_.\n",
    "\n",
    "2. Construcción de distintas redes neuronales:\n",
    "    - Con arquitecturas diferentes.\n",
    "    - Funciones de activación diferentes.\n",
    "    - Hiperparámetros diferentes. \n",
    "\n",
    "Tooodo esto para encontrar la mejor red neuronal, de hecho vamos a hacer el top 3 de las mejores redes neuronales y trataremos de deducir algunas concluciones de sus arquitecturas e hiperparámetros en relación con los datos. \n",
    "\n",
    "3. Comparación con los resultados de la red neuronal que solo hace uso de las características de la variable de predicción, con la red que hace uso de otros datos ambientales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bb2044c6e3b54dbca2fb5ed3ae7225df",
    "deepnote_app_coordinates": {
     "h": 30,
     "w": 12,
     "x": 0,
     "y": 434
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Vamos a hacer uso de las siguientes métricas:\n",
    "\n",
    "- MSE (Mean Squared Error)\n",
    "- MAE (Mean Absolute Error) \n",
    "- RMSE (Root Mean Squared Error) \n",
    "\n",
    "Estas métricas son apropiadas para evaluar el desempeño de un modelo en problemas de regresión, ya que miden la diferencia entre los valores predichos y los valores reales de la variable objetivo.\n",
    "\n",
    "MSE mide el promedio del cuadrado de los errores, mientras que MAE mide el promedio de los errores en valor absoluto. RMSE es similar a MSE, pero toma la raíz cuadrada del promedio de los errores al cuadrado.\n",
    "\n",
    "Además, son adecuadas para nuestro problema porque estamos interesados en predecir valores continuos, como la temperatura, la presión, la humedad y la luz. Y también, son fáciles de entender y comunicar, y permiten comparar el desempeño de diferentes modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ff9b19c4-718d-4116-a04b-62cd53a598e5' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_app_layout": "article",
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "21c5c2f1ab3e47cebaa2d02235fe75bb",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
