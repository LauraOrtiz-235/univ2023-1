{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7d48f1d98987473687b3599872f47aad",
    "deepnote_app_coordinates": {
     "h": 14,
     "w": 12,
     "x": 0,
     "y": 1
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "#  Proyecto 2  _(detecci贸n de emociones por voz)_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "de07fbdb77a142189107f3ad66505c96",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 16
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Integrantes\n",
    "\n",
    "Laura Sof铆a Ortiz Arcos\n",
    "David Santiago Fl贸rez Alsina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "43f0ed3755934f4ea0edfeb9a194e3ad",
    "deepnote_app_coordinates": {
     "h": 16,
     "w": 12,
     "x": 0,
     "y": 22
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5886,
    "execution_start": 1681601811736,
    "source_hash": "8a84a1e0"
   },
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn                 import preprocessing\n",
    "from torch.utils.data        import DataLoader\n",
    "from torch.utils.data        import TensorDataset\n",
    "from sklearn.preprocessing   import StandardScaler, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1c28773db58e467fa0ccd9bc044573b7",
    "deepnote_app_coordinates": {
     "h": 8,
     "w": 12,
     "x": 0,
     "y": 39
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Cargar y preparar los datos\n",
    "\n",
    "Para esta parte primero cargaremos los datos sobre se帽ales de voz, de acuerdo al sentimiento de dicha voz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "e86432980a69469191388ed4a4274fad",
    "deepnote_app_coordinates": {
     "h": 18,
     "w": 12,
     "x": 0,
     "y": 48
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 244,
    "execution_start": 1681601817626,
    "source_hash": "d270039a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>X</th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.187476</td>\n",
       "      <td>0.126197</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.869088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.137742</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>6.226562</td>\n",
       "      <td>6.140625</td>\n",
       "      <td>0.116586</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.195070</td>\n",
       "      <td>0.130847</td>\n",
       "      <td>0.243987</td>\n",
       "      <td>0.113140</td>\n",
       "      <td>1.191767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.121811</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.930339</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.914062</td>\n",
       "      <td>0.144983</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.062901</td>\n",
       "      <td>0.204945</td>\n",
       "      <td>0.131422</td>\n",
       "      <td>0.249978</td>\n",
       "      <td>0.118556</td>\n",
       "      <td>1.312690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.123758</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.332386</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.174115</td>\n",
       "      <td>0.129949</td>\n",
       "      <td>0.236967</td>\n",
       "      <td>0.107017</td>\n",
       "      <td>1.096409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.128469</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>1.012019</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>5.468750</td>\n",
       "      <td>5.382812</td>\n",
       "      <td>0.304910</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.156266</td>\n",
       "      <td>0.116783</td>\n",
       "      <td>0.216326</td>\n",
       "      <td>0.099543</td>\n",
       "      <td>1.386837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.109720</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.228795</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.306777</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  X  meanfreq        sd    median       Q25  \\\n",
       "0           0             1  1  0.181338  0.060495  0.187476  0.126197   \n",
       "1           1             2  2  0.186897  0.062260  0.195070  0.130847   \n",
       "2           2             3  3  0.189102  0.062901  0.204945  0.131422   \n",
       "3           4             5  5  0.183036  0.060051  0.174115  0.129949   \n",
       "4           5             6  6  0.168793  0.057910  0.156266  0.116783   \n",
       "\n",
       "        Q75       IQR      skew  ...  centroid   meanfun    minfun    maxfun  \\\n",
       "0  0.233586  0.107389  0.869088  ...  0.181338  0.137742  0.023022  0.271186   \n",
       "1  0.243987  0.113140  1.191767  ...  0.186897  0.121811  0.018412  0.271186   \n",
       "2  0.249978  0.118556  1.312690  ...  0.189102  0.123758  0.083333  0.262295   \n",
       "3  0.236967  0.107017  1.096409  ...  0.183036  0.128469  0.044693  0.258065   \n",
       "4  0.216326  0.099543  1.386837  ...  0.168793  0.109720  0.022472  0.235294   \n",
       "\n",
       "    meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.777344  0.085938  6.226562  6.140625  0.116586    sad  \n",
       "1  0.930339  0.085938  4.000000  3.914062  0.144983    sad  \n",
       "2  0.332386  0.085938  0.625000  0.539062  0.334783    sad  \n",
       "3  1.012019  0.085938  5.468750  5.382812  0.304910    sad  \n",
       "4  0.228795  0.093750  0.750000  0.656250  0.306777    sad  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('emotions_by_voice_registers.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bcd0c192fe6e408f87b87b0edf557aa3",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 67
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "_**Nos damos algo de informaci贸n sobre de qu茅 trata nuestro dataset y c贸mo es**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "cdd4e2cb41a84332b050aa71afc6ad61",
    "deepnote_app_coordinates": {
     "h": 27,
     "w": 12,
     "x": 0,
     "y": 72
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16,
    "execution_start": 1681601817903,
    "source_hash": "de1e323c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 909 entries, 0 to 908\n",
      "Data columns (total 24 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Unnamed: 0    909 non-null    int64  \n",
      " 1   Unnamed: 0.1  909 non-null    int64  \n",
      " 2   X             909 non-null    int64  \n",
      " 3   meanfreq      909 non-null    float64\n",
      " 4   sd            909 non-null    float64\n",
      " 5   median        909 non-null    float64\n",
      " 6   Q25           909 non-null    float64\n",
      " 7   Q75           909 non-null    float64\n",
      " 8   IQR           909 non-null    float64\n",
      " 9   skew          909 non-null    float64\n",
      " 10  kurt          909 non-null    float64\n",
      " 11  sp.ent        909 non-null    float64\n",
      " 12  sfm           909 non-null    float64\n",
      " 13  mode          909 non-null    float64\n",
      " 14  centroid      909 non-null    float64\n",
      " 15  meanfun       909 non-null    float64\n",
      " 16  minfun        909 non-null    float64\n",
      " 17  maxfun        909 non-null    float64\n",
      " 18  meandom       909 non-null    float64\n",
      " 19  mindom        909 non-null    float64\n",
      " 20  maxdom        909 non-null    float64\n",
      " 21  dfrange       909 non-null    float64\n",
      " 22  modindx       909 non-null    float64\n",
      " 23  label         909 non-null    object \n",
      "dtypes: float64(20), int64(3), object(1)\n",
      "memory usage: 170.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "82e55d3da4534ee6a761f0ceefcd639c",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 100
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "_**Luego eliminamos los atributos tipo 铆ndice, en este caso son:**_ 'Unnamed 0', 'Unnamed 0.1' y 'X'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "b8da34bf4d364418bce74e4e4ae19c48",
    "deepnote_app_coordinates": {
     "h": 26,
     "w": 12,
     "x": 0,
     "y": 105
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 122,
    "execution_start": 1681601817963,
    "source_hash": "1f73118f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.187476</td>\n",
       "      <td>0.126197</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.869088</td>\n",
       "      <td>2.863717</td>\n",
       "      <td>0.923566</td>\n",
       "      <td>0.307220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.137742</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>6.226562</td>\n",
       "      <td>6.140625</td>\n",
       "      <td>0.116586</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.195070</td>\n",
       "      <td>0.130847</td>\n",
       "      <td>0.243987</td>\n",
       "      <td>0.113140</td>\n",
       "      <td>1.191767</td>\n",
       "      <td>3.878650</td>\n",
       "      <td>0.918848</td>\n",
       "      <td>0.298859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.121811</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.930339</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.914062</td>\n",
       "      <td>0.144983</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.062901</td>\n",
       "      <td>0.204945</td>\n",
       "      <td>0.131422</td>\n",
       "      <td>0.249978</td>\n",
       "      <td>0.118556</td>\n",
       "      <td>1.312690</td>\n",
       "      <td>4.589995</td>\n",
       "      <td>0.919519</td>\n",
       "      <td>0.313069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.123758</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.332386</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.174115</td>\n",
       "      <td>0.129949</td>\n",
       "      <td>0.236967</td>\n",
       "      <td>0.107017</td>\n",
       "      <td>1.096409</td>\n",
       "      <td>3.680995</td>\n",
       "      <td>0.921361</td>\n",
       "      <td>0.329295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.128469</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>1.012019</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>5.468750</td>\n",
       "      <td>5.382812</td>\n",
       "      <td>0.304910</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.156266</td>\n",
       "      <td>0.116783</td>\n",
       "      <td>0.216326</td>\n",
       "      <td>0.099543</td>\n",
       "      <td>1.386837</td>\n",
       "      <td>5.031744</td>\n",
       "      <td>0.926238</td>\n",
       "      <td>0.337047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.109720</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.228795</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.306777</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>0.244013</td>\n",
       "      <td>0.035477</td>\n",
       "      <td>0.254385</td>\n",
       "      <td>0.229653</td>\n",
       "      <td>0.265573</td>\n",
       "      <td>0.035920</td>\n",
       "      <td>2.214752</td>\n",
       "      <td>7.565052</td>\n",
       "      <td>0.821874</td>\n",
       "      <td>0.136933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244013</td>\n",
       "      <td>0.202433</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.616536</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>1.609375</td>\n",
       "      <td>1.398438</td>\n",
       "      <td>0.281869</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>0.235383</td>\n",
       "      <td>0.045303</td>\n",
       "      <td>0.248974</td>\n",
       "      <td>0.220745</td>\n",
       "      <td>0.264233</td>\n",
       "      <td>0.043488</td>\n",
       "      <td>2.474743</td>\n",
       "      <td>9.959019</td>\n",
       "      <td>0.848109</td>\n",
       "      <td>0.236957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235383</td>\n",
       "      <td>0.189293</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.115723</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>5.234375</td>\n",
       "      <td>0.167861</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>0.231211</td>\n",
       "      <td>0.044793</td>\n",
       "      <td>0.234847</td>\n",
       "      <td>0.221477</td>\n",
       "      <td>0.262090</td>\n",
       "      <td>0.040613</td>\n",
       "      <td>2.607668</td>\n",
       "      <td>10.698821</td>\n",
       "      <td>0.848702</td>\n",
       "      <td>0.241998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231211</td>\n",
       "      <td>0.171805</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.070801</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>4.554688</td>\n",
       "      <td>4.289062</td>\n",
       "      <td>0.214936</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>0.213587</td>\n",
       "      <td>0.082267</td>\n",
       "      <td>0.249435</td>\n",
       "      <td>0.207680</td>\n",
       "      <td>0.268538</td>\n",
       "      <td>0.060858</td>\n",
       "      <td>3.460579</td>\n",
       "      <td>18.034614</td>\n",
       "      <td>0.882544</td>\n",
       "      <td>0.425394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213587</td>\n",
       "      <td>0.155277</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.724888</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>6.812500</td>\n",
       "      <td>6.539062</td>\n",
       "      <td>0.238857</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.078746</td>\n",
       "      <td>0.245034</td>\n",
       "      <td>0.209794</td>\n",
       "      <td>0.264031</td>\n",
       "      <td>0.054238</td>\n",
       "      <td>2.563983</td>\n",
       "      <td>10.392885</td>\n",
       "      <td>0.887389</td>\n",
       "      <td>0.404993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.162938</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.915625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.281250</td>\n",
       "      <td>6.281250</td>\n",
       "      <td>0.193141</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>909 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     meanfreq        sd    median       Q25       Q75       IQR      skew  \\\n",
       "0    0.181338  0.060495  0.187476  0.126197  0.233586  0.107389  0.869088   \n",
       "1    0.186897  0.062260  0.195070  0.130847  0.243987  0.113140  1.191767   \n",
       "2    0.189102  0.062901  0.204945  0.131422  0.249978  0.118556  1.312690   \n",
       "3    0.183036  0.060051  0.174115  0.129949  0.236967  0.107017  1.096409   \n",
       "4    0.168793  0.057910  0.156266  0.116783  0.216326  0.099543  1.386837   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "904  0.244013  0.035477  0.254385  0.229653  0.265573  0.035920  2.214752   \n",
       "905  0.235383  0.045303  0.248974  0.220745  0.264233  0.043488  2.474743   \n",
       "906  0.231211  0.044793  0.234847  0.221477  0.262090  0.040613  2.607668   \n",
       "907  0.213587  0.082267  0.249435  0.207680  0.268538  0.060858  3.460579   \n",
       "908  0.212537  0.078746  0.245034  0.209794  0.264031  0.054238  2.563983   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n",
       "0     2.863717  0.923566  0.307220  ...  0.181338  0.137742  0.023022   \n",
       "1     3.878650  0.918848  0.298859  ...  0.186897  0.121811  0.018412   \n",
       "2     4.589995  0.919519  0.313069  ...  0.189102  0.123758  0.083333   \n",
       "3     3.680995  0.921361  0.329295  ...  0.183036  0.128469  0.044693   \n",
       "4     5.031744  0.926238  0.337047  ...  0.168793  0.109720  0.022472   \n",
       "..         ...       ...       ...  ...       ...       ...       ...   \n",
       "904   7.565052  0.821874  0.136933  ...  0.244013  0.202433  0.028829   \n",
       "905   9.959019  0.848109  0.236957  ...  0.235383  0.189293  0.031250   \n",
       "906  10.698821  0.848702  0.241998  ...  0.231211  0.171805  0.022346   \n",
       "907  18.034614  0.882544  0.425394  ...  0.213587  0.155277  0.020592   \n",
       "908  10.392885  0.887389  0.404993  ...  0.212537  0.162938  0.024845   \n",
       "\n",
       "       maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0    0.271186  0.777344  0.085938  6.226562  6.140625  0.116586    sad  \n",
       "1    0.271186  0.930339  0.085938  4.000000  3.914062  0.144983    sad  \n",
       "2    0.262295  0.332386  0.085938  0.625000  0.539062  0.334783    sad  \n",
       "3    0.258065  1.012019  0.085938  5.468750  5.382812  0.304910    sad  \n",
       "4    0.235294  0.228795  0.093750  0.750000  0.656250  0.306777    sad  \n",
       "..        ...       ...       ...       ...       ...       ...    ...  \n",
       "904  0.271186  0.616536  0.210938  1.609375  1.398438  0.281869  happy  \n",
       "905  0.275862  1.115723  0.265625  5.500000  5.234375  0.167861  happy  \n",
       "906  0.275862  1.070801  0.265625  4.554688  4.289062  0.214936  happy  \n",
       "907  0.275862  1.724888  0.273438  6.812500  6.539062  0.238857  happy  \n",
       "908  0.258065  0.915625  0.000000  6.281250  6.281250  0.193141  happy  \n",
       "\n",
       "[909 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(labels = ['Unnamed: 0', 'Unnamed: 0.1', 'X'], axis = 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "070e68d033094a4aa0e804ec8d7f0651",
    "deepnote_app_coordinates": {
     "h": 6,
     "w": 12,
     "x": 0,
     "y": 132
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Nos hacemos unas _**funciones para estandarizar**_ y probar el rendimiento del modelo con las distintas estandarizaciones, y sin alguna estandarizaci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "3f01408ec282481d8a460da536faf0d5",
    "deepnote_app_coordinates": {
     "h": 24,
     "w": 12,
     "x": 0,
     "y": 139
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1681601818098,
    "source_hash": "48e28569"
   },
   "outputs": [],
   "source": [
    "def data_scaling(data: pd.DataFrame,\n",
    "                 scaler: sklearn.preprocessing._data,\n",
    "                 scaling_columns     = [], \n",
    "                 not_scaling_columns = []):\n",
    "    \n",
    "    \"\"\"\n",
    "    Scales the specified columns in the input DataFrame using the given scaler object.\n",
    "    \n",
    "    INPUT:\n",
    "        - data:                Pandas DataFrame to be scaled\n",
    "        - scaler:              Scaler object from sklearn.preprocessing\n",
    "        - scaling_columns:     List of column names to scale\n",
    "        - not_scaling_columns: List of column names to not scale\n",
    "        \n",
    "    OUTPUT: \n",
    "        - pd.DataFrame:        A scaled copy of the input data with the specified columns scaled\n",
    "    \"\"\"\n",
    "\n",
    "    # if the user provided all the columns he wishes to scale \n",
    "    if scaling_columns:\n",
    "        \n",
    "        data[scaling_columns].iloc[:] = scaler.fit_transform(data[scaling_columns].iloc[:].to_numpy())\n",
    "\n",
    "    # if user provided only the columns he doesn't want to scale\n",
    "    elif not_scaling_columns:\n",
    "        \n",
    "        used_cols = data.columns.difference(not_scaling_columns)\n",
    "        data.loc[:, used_cols] = scaler.fit_transform(data.loc[:, used_cols].to_numpy())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "3ce91b92b2f94623950c9131c3b8f056",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 164
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 50,
    "execution_start": 1681601818147,
    "source_hash": "79350299"
   },
   "outputs": [],
   "source": [
    "X  = data_scaling(data = X, scaler = StandardScaler(), not_scaling_columns = ['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "60d4dbe161964c4a97f2cb71bbbf8cd9",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 169
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Tambi茅n _**creamos variables dummy**_ para la variable categ贸rica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "6cef66c936b64a2cb3e87f442fb4ae62",
    "deepnote_app_coordinates": {
     "h": 27,
     "w": 12,
     "x": 0,
     "y": 174
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1681601818196,
    "source_hash": "112e4862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 909 entries, 0 to 908\n",
      "Data columns (total 23 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   meanfreq     909 non-null    float64\n",
      " 1   sd           909 non-null    float64\n",
      " 2   median       909 non-null    float64\n",
      " 3   Q25          909 non-null    float64\n",
      " 4   Q75          909 non-null    float64\n",
      " 5   IQR          909 non-null    float64\n",
      " 6   skew         909 non-null    float64\n",
      " 7   kurt         909 non-null    float64\n",
      " 8   sp.ent       909 non-null    float64\n",
      " 9   sfm          909 non-null    float64\n",
      " 10  mode         909 non-null    float64\n",
      " 11  centroid     909 non-null    float64\n",
      " 12  meanfun      909 non-null    float64\n",
      " 13  minfun       909 non-null    float64\n",
      " 14  maxfun       909 non-null    float64\n",
      " 15  meandom      909 non-null    float64\n",
      " 16  mindom       909 non-null    float64\n",
      " 17  maxdom       909 non-null    float64\n",
      " 18  dfrange      909 non-null    float64\n",
      " 19  modindx      909 non-null    float64\n",
      " 20  label_angry  909 non-null    uint8  \n",
      " 21  label_happy  909 non-null    uint8  \n",
      " 22  label_sad    909 non-null    uint8  \n",
      "dtypes: float64(20), uint8(3)\n",
      "memory usage: 144.8 KB\n"
     ]
    }
   ],
   "source": [
    "X = pd.get_dummies(X, columns = ['label'])\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a7d9df7326d9499996b207472c7a3b9e",
    "deepnote_app_coordinates": {
     "h": 20,
     "w": 12,
     "x": 0,
     "y": 202
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Creaci贸n de los conjuntos de entrenamiento, validaci贸n y test\n",
    "\n",
    "Ahora, creamos los conjuntos de entrenamiento, validaci贸n y test. Vamos a usar una proporci贸n de:\n",
    "\n",
    "- Para test un 10%.\n",
    "- Para validaci贸n un 10%.\n",
    "- Y el resto para train.\n",
    "\n",
    "Tambi茅n creamos una _**clase Dataset**_ que nos prepara los datos para alimentar la red neuronal, convirtiendolos a parejas ordenadas de tensores conteniento los atributos y la variable objetivo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "c4f06243d30447bbbe7e60dce0194f36",
    "deepnote_app_coordinates": {
     "h": 15,
     "w": 12,
     "x": 0,
     "y": 223
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1681601818247,
    "source_hash": "10f72150"
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    " \n",
    "  def __init__(self,\n",
    "               df: pd.DataFrame,\n",
    "               target_column: list):\n",
    "\n",
    "    y = df[target_column].values\n",
    "    X = df.drop(target_column,axis=1).values\n",
    "\n",
    "    self.X = torch.tensor(X, dtype=torch.float32)\n",
    "    self.y = torch.tensor(y, dtype=torch.float32)\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.y)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "b6fc1e2414864e14bb8f067f5f8dc423",
    "deepnote_app_coordinates": {
     "h": 25,
     "w": 12,
     "x": 0,
     "y": 239
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1681601818247,
    "source_hash": "26905485"
   },
   "outputs": [],
   "source": [
    "def train_test_val_split(data: pd.DataFrame, \n",
    "                         proportions = [0.8, 0.1, 0.1]):\n",
    "\n",
    "    \"\"\"\n",
    "        Splits the input DataFrame into train, test, and validation sets based on the given proportions.\n",
    "\n",
    "        INPUT:\n",
    "            - data:        The dataframe to be splitted\n",
    "\n",
    "            - proportions: List of proportions of size of each partition of the dataframe, first element \n",
    "                           of the list corresponds to the proportion of train, the 2nd one indicates the\n",
    "                           test proportion, and the last element is the validation proportion\n",
    "\n",
    "        OUTPUT: \n",
    "            - (pd.DataFrame, pd.DataFrame, pd.DataFrame): in the form\n",
    "              (train_dataset, test_dataset, validation_dataset)\n",
    "    \"\"\"\n",
    "\n",
    "    data_len = len(data)\n",
    "\n",
    "    #end of train\n",
    "    train_indx = int(proportions[0]*data_len)\n",
    "\n",
    "    #end of test\n",
    "    test_indx = train_indx + int(proportions[1]*data_len)\n",
    "\n",
    "    out = (data.iloc[:train_indx],\n",
    "           data.iloc[train_indx+1:test_indx],\n",
    "           data.iloc[test_indx+1:-1])\n",
    "           \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "4057a28ad8514c9dbd8a305bd41c2726",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 265
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 61,
    "execution_start": 1681601818247,
    "source_hash": "76f20659"
   },
   "outputs": [],
   "source": [
    "train, test, val = train_test_val_split(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "175fac5824f940b9baac432d39254692",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 270
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Preparaci贸n de los datos para alimentar la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "64525e52d60e40e5af7621b758899476",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 276
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Continuamos preparando los datos, convirtiendolos a tensores, y creando los dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "9e812cbbb21440c2a9eae01dfb08860b",
    "deepnote_app_coordinates": {
     "h": 7,
     "w": 12,
     "x": 0,
     "y": 281
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 25,
    "execution_start": 1681601818285,
    "source_hash": "ffd58a99"
   },
   "outputs": [],
   "source": [
    "outcome_vars = ['label_angry', 'label_happy', 'label_sad']\n",
    "\n",
    "# Use the Dataset class to prepare each array in the form of tensors\n",
    "train_sec = Dataset(train, outcome_vars)\n",
    "test_sec  = Dataset(test, outcome_vars)\n",
    "val_sec   = Dataset(val, outcome_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f86a952f2ac84bdebc016f4dfe5a9690",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 289
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Acontinuaci贸n puede ver el tama帽o de nuestro set de entrenamiento, testeo y validaci贸n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "85f11a073b0b4db1bed8ac0bfa71415f",
    "deepnote_app_coordinates": {
     "h": 6,
     "w": 12,
     "x": 0,
     "y": 295
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1681601818306,
    "source_hash": "33ef3267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: torch.Size([727, 20]), test: torch.Size([89, 20]), val: torch.Size([90, 20])\n"
     ]
    }
   ],
   "source": [
    "s = f\"train: {train_sec.X.shape}, test: {test_sec.X.shape}, val: {val_sec.X.shape}\"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "2286ef002c0d4fbe83b0cecf22a34ffa",
    "deepnote_app_coordinates": {
     "h": 20,
     "w": 12,
     "x": 0,
     "y": 302
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1681601818314,
    "source_hash": "3bc8e5a4"
   },
   "outputs": [],
   "source": [
    "# Define the DataLoaders to load the information in batches\n",
    "train_data = DataLoader(\n",
    "    train_sec,\n",
    "    batch_size = 16,\n",
    "    shuffle    = False,\n",
    " )\n",
    "\n",
    "test_data = DataLoader(\n",
    "    test_sec,\n",
    "    batch_size  = 16,\n",
    "    shuffle     = False,\n",
    "    #num_workers = 0,\n",
    "    #collate_fn  = None,\n",
    "    #pin_memory  = False,\n",
    " )\n",
    "\n",
    "val_data = DataLoader(\n",
    "    val_sec,\n",
    "    batch_size  = 16,\n",
    "    shuffle     = False,\n",
    "    #num_workers = 0,\n",
    "    #collate_fn  = None,\n",
    "    #pin_memory  = False,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "27f561315fb1498980f18ee9e909abe2",
    "deepnote_app_coordinates": {
     "h": 8,
     "w": 12,
     "x": 0,
     "y": 323
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Definici贸n de la clase Net, el optimizador, la funci贸n de costo y el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "26257ae4f58e4094924139c627090bf1",
    "deepnote_app_coordinates": {
     "h": 49,
     "w": 12,
     "x": 0,
     "y": 332
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Para hacer nuestra red neuronal decidimos darle la posibilidad de generar distintas estructuras para ella, nuestro m茅todo para hacer esto automatizadamente consiste guiarnos de una funci贸n (seleccionada por nosotros), para definir la cantidad de neuronas que queremos tener en el n煤mero de capas que hayamos decidido poner.\n",
    "\n",
    "Si por ejemplo queremos una arquitectura que empieza con su primera capa con 20 neuronas, aumenta a 40 y sigue en 40 y despu茅s baja a 20 y llega hasta 3 podr铆amos guiarnos de esta funci贸n e ir mapeando ciertos puntos de ella:\n",
    "\n",
    "\n",
    "<iframe src=\"https://www.desmos.com/calculator/dy1goxwgu2?embed\" width=\"800\" height=\"400\" style=\"border: 1px solid #ccc\" frameborder=50></iframe> (hay un link con la gr谩fica, pero no sabemos porque Jupyter no la muestra. Igual en el .html se puede ver la gr谩fica por si acaso)\n",
    "\n",
    "\n",
    "En este caso estamos usando la funci贸n \n",
    "\n",
    "$$f\\left(x\\right)\\ =\\ 10\\cdot\\sqrt{x}\\ -\\ x\\ +\\ 20\\ \\left\\{0\\le x\\le130\\right\\}$$\n",
    "\n",
    "La cu谩l fue definida a nuestro antojo para cumplir lo que quer铆amos con nuestra red neuronal, que b谩sicamente es que el n煤mero de neuronas por capa aumente un poquito y luego caiga hasta llegar al n煤mero de neuronas de salida. Con esta funci贸n definida lo que resta es evaluar el conjunto \n",
    "\n",
    "$$\\vec{x} = \\{ i*\\Delta X,\\quad i \\in \\mathbb{Z}^{*}, \\quad \\Delta X = \\frac{\\text{inicio dominio} - \\text{fin  dominio}}{\\text{num. capas}} \\}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "4d095fed47db45dab4a28f9ac7034b9e",
    "deepnote_app_coordinates": {
     "h": 29,
     "w": 12,
     "x": 0,
     "y": 382
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1681601818338,
    "source_hash": "ce280208"
   },
   "outputs": [],
   "source": [
    "def g1(domain: list)-> np.array:\n",
    "    \"\"\"\n",
    "    start at 20, grow and then decay function\n",
    "\n",
    "    INPUT:\n",
    "        - domain:   List of numeric values to be processed\n",
    "    OUTPUT: \n",
    "        - np.array: Array of integers resulting from the application of the mathematical formula\n",
    "    \"\"\"\n",
    "\n",
    "    return ((10*np.sqrt(domain)) - domain + 20).astype(np.int32)\n",
    "\n",
    "\n",
    "def g2(domain: list) -> np.array:\n",
    "    \"\"\"\n",
    "    f(x) = 20 -x, kind of function\n",
    "    \n",
    "    INPUT:\n",
    "        - domain:   list of numeric values to be processed\n",
    "    OUTPUT: \n",
    "        - np.array: array of integers resulting from the application of the mathematical formula\n",
    "    \"\"\"\n",
    "    \n",
    "    # apply the mathematical formula to each item in the list and convert the resulting array to integers\n",
    "    return (20 - (0.05 * np.array(domain) ** 3)).astype(np.int32)\n",
    "\n",
    "\n",
    "def g3(domain: list)-> np.array:\n",
    "    \"\"\"\n",
    "    Constant function.\n",
    "    \n",
    "    INPUT:\n",
    "        - domain:   list of numeric values (not used in this function)\n",
    "    OUTPUT: \n",
    "        - np.array: array of integers where each element is 20\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([20 for i in range(len(domain))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ab406ebf62cb4263aa1c5770d93bd2ab",
    "deepnote_app_coordinates": {
     "h": 9,
     "w": 12,
     "x": 0,
     "y": 412
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Con nuestra abstracci贸n anterior, podemos proseguir a crear diferentes modelos simplemente cambiando la funci贸n del comportamiento del numero de neuronas. As铆 definimos una funci贸n para construir diferentes modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "25580519473d40b9bcde50e5b5cd75a9",
    "deepnote_app_coordinates": {
     "h": 44,
     "w": 12,
     "x": 0,
     "y": 422
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1681601818338,
    "source_hash": "ce0cf2d6"
   },
   "outputs": [],
   "source": [
    "def build_different_models(input_sz: int,\n",
    "                           output_sz: int,\n",
    "                           max_layers: int, \n",
    "                           envelope_f: callable, \n",
    "                           lim: int) -> list:\n",
    "\n",
    "    \"\"\"\n",
    "    Builds a list of different neural network models with varying numbers of hidden layers and neurons.\n",
    "    \n",
    "    INPUT:\n",
    "        - input_sz:     integer specifying the size of the input layer.\n",
    "        - output_sz:    integer specifying the size of the output layer.\n",
    "        - max_layers:   integer specifying the maximum number of hidden layers allowed.\n",
    "        - envelope_f:   callable that takes in an array of sample points and returns an array of integers\n",
    "                        specifying the number of neurons in each hidden layer of a neural network model.\n",
    "        - lim:          integer specifying the maximum value of the sample points array, this is the \n",
    "                        limit of the domain of the function.\n",
    "    \n",
    "    OUTPUT: \n",
    "        - list of nn.ModuleList: list of different neural network models with varying architectures\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # list of lists, each list contains the number of neurons\n",
    "    # the model will have on each layer\n",
    "    hidden_layers_structure = []\n",
    "\n",
    "\n",
    "    # here we build the different neural architectures\n",
    "    for i in range(2, max_layers+1):\n",
    "        #delta = int((input_sz - output_sz)/i)\n",
    "        sample_points = np.linspace(0, lim, i)\n",
    "        structure     = envelope_f(sample_points) \n",
    "        hidden_layers_structure.append(structure)\n",
    "\n",
    "    # list of models\n",
    "    models = []\n",
    "\n",
    "    # populate the list of models\n",
    "    for structure in hidden_layers_structure:\n",
    "\n",
    "        # Define your layers for the model \n",
    "        model = nn.ModuleList()\n",
    "\n",
    "        # first layer\n",
    "        model.append(nn.Linear(input_sz, structure[0]))\n",
    "\n",
    "        # hidden layers\n",
    "        for i, n in enumerate(structure[:-1]):\n",
    "            model.append(nn.Linear(structure[i], structure[i+1]))\n",
    "\n",
    "        \n",
    "        # output layer\n",
    "        model.append(nn.Linear(structure[-1], output_sz))\n",
    "        model.append(nn.Linear(3, output_sz))\n",
    "\n",
    "        models.append(model)\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "96440bdd9ff64a5d9caca4e21f67c725",
    "deepnote_app_coordinates": {
     "h": 7,
     "w": 12,
     "x": 0,
     "y": 527
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Ahora continuamos definiendo nuestra _**clase NeuralNetwork**_ con la estructura de la red neuronal (haciendo uso de capas lineales y las diferentes funciones de activaci贸n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "70de3fffba704b3ba0f465366a36e753",
    "deepnote_app_coordinates": {
     "h": 59,
     "w": 12,
     "x": 0,
     "y": 467
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 100,
    "execution_start": 1681601818339,
    "source_hash": "fcb2672e"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "        A class that defines a neural network with a configurable number of layers\n",
    "        and activation function.\n",
    "\n",
    "        Attributes:\n",
    "            - flatten (nn.Flatten):   Flattens the input to a 1D tensor.\n",
    "            - model (nn.ModuleList):  Contains the layers of the neural network.\n",
    "            - output (nn.Softmax):    Applies softmax function to the output of the last layer.\n",
    "            - activation (nn.Module): Activation function used in the hidden layers.\n",
    "\n",
    "        Methods:\n",
    "            - __init__():             Initializes the NeuralNetwork class.\n",
    "            - forward(x):             Forward pass through the neural network.\n",
    "            - set_model(mod):         Sets the model layers.\n",
    "            - set_activation(act):    Sets the activation function used in the hidden layers.\n",
    "            - make_prediction(x):     Generates predictions for a given input.\n",
    "\n",
    "        Example:\n",
    "            >>> model = NeuralNetwork()\n",
    "            >>> model.set_activation(nn.LeakyReLU())\n",
    "            >>> model.set_model([nn.Linear(10, 20), nn.Linear(20, 2)])\n",
    "            >>> predictions = model.make_prediction(torch.randn(5, 10).numpy())\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.model   = nn.ModuleList()\n",
    "\n",
    "        self.output     = nn.Softmax()\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the neural network.\n",
    "\n",
    "        Args:\n",
    "            - x (torch.Tensor):      Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            - output (torch.Tensor): Output tensor of the neural network.\n",
    "        \"\"\"\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        for layer in self.model:\n",
    "            x = self.activation(layer(x))\n",
    "\n",
    "        return self.output(x)\n",
    "    \n",
    "    def set_model(self, mod):\n",
    "        \"\"\"\n",
    "        Sets the model layers.\n",
    "\n",
    "        Args:\n",
    "            - mod (list): List containing the neural network layers.\n",
    "        \"\"\"\n",
    "        self.model = mod\n",
    "\n",
    "    def set_activation(self, act: torch.nn.modules.activation):\n",
    "        \"\"\"\n",
    "        Sets the activation function used in the hidden layers.\n",
    "\n",
    "        Args:\n",
    "            - act (nn.Module): Activation function.\n",
    "        \"\"\"\n",
    "        self.activation = act\n",
    "\n",
    "    def make_prediction(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Generates predictions for a given input.\n",
    "\n",
    "        Args:\n",
    "            - x (np.array):     Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            - preds (np.array): Predictions tensor of the neural network.\n",
    "        \"\"\"\n",
    "        preds = self.forward(x).argmax(dim = 1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6bf6e58905244e899c3d331b983c442b",
    "deepnote_app_coordinates": {
     "h": 3,
     "w": 12,
     "x": 0,
     "y": 535
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Imprimimos nuestros posibles candidatos a modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "f1895027b44a4174ace152281a3decf5",
    "deepnote_app_coordinates": {
     "h": 30,
     "w": 12,
     "x": 0,
     "y": 539
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 61,
    "execution_start": 1681601818379,
    "source_hash": "d8e62acc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModuleList(\n",
       "   (0): Linear(in_features=20, out_features=20, bias=True)\n",
       "   (1): Linear(in_features=20, out_features=4, bias=True)\n",
       "   (2): Linear(in_features=4, out_features=3, bias=True)\n",
       "   (3): Linear(in_features=3, out_features=3, bias=True)\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): Linear(in_features=20, out_features=20, bias=True)\n",
       "   (1): Linear(in_features=20, out_features=35, bias=True)\n",
       "   (2): Linear(in_features=35, out_features=4, bias=True)\n",
       "   (3): Linear(in_features=4, out_features=3, bias=True)\n",
       "   (4): Linear(in_features=3, out_features=3, bias=True)\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): Linear(in_features=20, out_features=20, bias=True)\n",
       "   (1): Linear(in_features=20, out_features=42, bias=True)\n",
       "   (2): Linear(in_features=42, out_features=26, bias=True)\n",
       "   (3): Linear(in_features=26, out_features=4, bias=True)\n",
       "   (4): Linear(in_features=4, out_features=3, bias=True)\n",
       "   (5): Linear(in_features=3, out_features=3, bias=True)\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): Linear(in_features=20, out_features=20, bias=True)\n",
       "   (1): Linear(in_features=20, out_features=44, bias=True)\n",
       "   (2): Linear(in_features=44, out_features=35, bias=True)\n",
       "   (3): Linear(in_features=35, out_features=21, bias=True)\n",
       "   (4): Linear(in_features=21, out_features=4, bias=True)\n",
       "   (5): Linear(in_features=4, out_features=3, bias=True)\n",
       "   (6): Linear(in_features=3, out_features=3, bias=True)\n",
       " )]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_models = build_different_models(input_sz   = 20,\n",
    "                                          output_sz  = 3,\n",
    "                                          max_layers = 5, \n",
    "                                          envelope_f = g1, \n",
    "                                          lim = 130)\n",
    "candidate_models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1373921b5ac64d53bc1de1afa59e1d97",
    "deepnote_app_coordinates": {
     "h": 3,
     "w": 12,
     "x": 0,
     "y": 570
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Revisamos que estemos usando GPU y definimos el dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "bd2aa6faac274eef96148391d781d558",
    "deepnote_app_coordinates": {
     "h": 8,
     "w": 12,
     "x": 0,
     "y": 574
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 22,
    "execution_start": 1681601818419,
    "source_hash": "5600cde9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the GPU available? True\n",
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "gpu_avail = torch.cuda.is_available()\n",
    "print(f\"Is the GPU available? {gpu_avail}\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "46f40a283f014c4f9ddbd97e204de852",
    "deepnote_app_coordinates": {
     "h": 3,
     "w": 12,
     "x": 0,
     "y": 583
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Luego definimos el modelo, el optimizador y la funci贸n de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_id": "4f8c9006db474248b1fde6fd569dad8b",
    "deepnote_app_coordinates": {
     "h": 8,
     "w": 12,
     "x": 0,
     "y": 587
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1681601818479,
    "source_hash": "4696ac4f"
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "model.set_model(candidate_models[0])\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "837f1ed7ce9b4676ba8b0a89c69a58f0",
    "deepnote_app_coordinates": {
     "h": 11,
     "w": 12,
     "x": 0,
     "y": 596
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Entrenando la red y analizando el mejo modelo\n",
    "\n",
    "Luego de haber preparado los datos, definir la arquitectura, y de as铆 entrenar la red, vamos a definir la _**funci贸n de entrenamiento**_ para guardar nuestro mejor modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_id": "2549c548da674a0299e0a4ef887a9d65",
    "deepnote_app_coordinates": {
     "h": 63,
     "w": 12,
     "x": 0,
     "y": 608
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 48,
    "execution_start": 1681601818479,
    "source_hash": "3cad2daf"
   },
   "outputs": [],
   "source": [
    "# pass the model to the GPU device\n",
    "model.to(device)\n",
    "\n",
    "def train_model(model,\n",
    "                optimizer,\n",
    "                loss_module,\n",
    "                train_loader,\n",
    "                valid_loader, \n",
    "                num_epochs):\n",
    "    \"\"\"\n",
    "    Trains the specified model using the given data and hyperparameters, and saves the model with \n",
    "    the smallest validation error.\n",
    "\n",
    "    INPUT:\n",
    "        - model:        the PyTorch model to be trained\n",
    "        - optimizer:    the PyTorch optimizer used to update the model's parameters\n",
    "        - loss_module:  the PyTorch loss function used to calculate the training loss\n",
    "        - train_loader: the PyTorch DataLoader containing the training data\n",
    "        - valid_loader: the PyTorch DataLoader containing the validation data\n",
    "        - num_epochs:   the number of epochs to train the model for\n",
    "\n",
    "    OUTPUT:\n",
    "        None\n",
    "    \"\"\"\n",
    "  \n",
    "    # Let's find the smallest validation error value.\n",
    "    # So initialize it to 'infinity'\n",
    "    valid_loss_min = np.inf  \n",
    "  \n",
    "    for i in range(num_epochs):\n",
    "      \n",
    "        # We put the model in training mode.        \n",
    "        # It is important in other architectures such as convolutional networks.\n",
    "        model.train()  \n",
    "        train_loss = 0.0\n",
    "        v_loss = 0.0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            # move the attribute and label tensors to the GPU device\n",
    "            inputs, labels = data.to(device), target.to(device)\n",
    "            \n",
    "            # reset the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: calculate the output for the input data.\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = loss_module(outputs, labels)\n",
    "\n",
    "            # backpropagation: gradient calculation\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # update cost account across batches\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        train_loss = train_loss/len(train_loader.dataset) \n",
    "\n",
    "        # We put the model in evaluation mode.\n",
    "        model.eval() \n",
    "\n",
    "        # we are going to evaluate the trained model, calculating predictions with the validation set\n",
    "        for data,target in valid_loader:\n",
    "            data=data.to(device)\n",
    "            target=target.to(device)\n",
    "            output=model(data)\n",
    "            valid_loss= loss_module(output, target)\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    \n",
    "        # print training and validation statistics\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            i, train_loss, valid_loss))\n",
    "        \n",
    "\n",
    "        # We save the model with the smallest validation error.\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), 'model_bikeshare.pt')\n",
    "            valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ce2ac1b0490e41d590a8552ed430a89e",
    "deepnote_app_coordinates": {
     "h": 3,
     "w": 12,
     "x": 0,
     "y": 672
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Imprimimos los par谩metros de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cell_id": "4a9e2dbea2104287ba373d8dff218b86",
    "deepnote_app_coordinates": {
     "h": 30,
     "w": 12,
     "x": 0,
     "y": 676
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6090,
    "execution_start": 1681601838239,
    "source_hash": "fbe0d70e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5201/2872644584.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (inf --> 0.137598).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.098303 \tValidation Loss: 0.137598\n",
      "Validation loss decreased (0.137598 --> 0.137598).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.set_model(candidate_models[3])\n",
    "optimizer.zero_grad()\n",
    "\n",
    "train_model(model, optimizer, criterion, train_data, val_data, 50)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bb26c5d8bc5d4c67804d05e7b9dcc2d1",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 707
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Y finalmente, evaluamos el desempe帽o del modelo que creemos que es mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cell_id": "b8493c18318445c1912700fc6014e7aa",
    "deepnote_app_coordinates": {
     "h": 22,
     "w": 12,
     "x": 0,
     "y": 712
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 33,
    "execution_start": 1681601844297,
    "source_hash": "4796fe0d"
   },
   "outputs": [],
   "source": [
    "def test_model_performance(model: NeuralNetwork, test_set: Dataset): \n",
    "    \"\"\"\n",
    "    Evaluates the performance of a neural network model on a given test dataset.\n",
    "\n",
    "    INPUT:\n",
    "        - model:    A neural network model to be tested.\n",
    "        - test_set: A dataset containing test samples and corresponding labels.\n",
    "\n",
    "    OUTPUT:\n",
    "        - accuracy: The accuracy of the model on the test dataset, expressed as a percentage.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # we feed the model with the data in order to get the outputs\n",
    "    # and with all the outputs predicted by de model, we proceed\n",
    "    # to get the maximum of each prediction category\n",
    "    preds = model(test_set.X).argmax(dim = 1)\n",
    "\n",
    "    # now we compare with the expected outcomes\n",
    "    target = test_set.y.argmax(dim = 1)\n",
    "\n",
    "    # we compare both and get the total number of correct answers given \n",
    "    # by the model\n",
    "    correct = np.sum( np.array((preds == target).flatten()) ).astype('int32')\n",
    "    accuracy = correct/len(preds) \n",
    "\n",
    "    return accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cell_id": "cdb29fdcd0734e7ba7950a3bce29ab61",
    "deepnote_app_coordinates": {
     "h": 8,
     "w": 12,
     "x": 0,
     "y": 735
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 22,
    "execution_start": 1681601844318,
    "source_hash": "ede0edf2"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5201/3143951657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5201/1127639756.py\u001b[0m in \u001b[0;36mtest_model_performance\u001b[0;34m(model, test_set)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# and with all the outputs predicted by de model, we proceed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# to get the maximum of each prediction category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# now we compare with the expected outcomes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5201/2872644584.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "test_model_performance(model, test_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "aafcc4ed2b754aa9afa8aed8adf2e6a0",
    "deepnote_app_coordinates": {
     "h": 14,
     "w": 12,
     "x": 0,
     "y": 744
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Mejoras para el modelo\n",
    "\n",
    "Como se puede observar anteriormente, no tenemos un buen accuracy como estamos definiendo nuestro modelo, por lo anterior decidimos crear una funci贸n que nos ayudara a ver cu谩l modelo ser铆a el mejor, para esto, la funci贸n nos permite crear diferentes arquitecturas de la red neuronal, y tambi茅n utilizar varias funciones de activaci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "cd92cbd427f7458882e665119ec0a2a0",
    "deepnote_app_coordinates": {
     "h": 61,
     "w": 12,
     "x": 0,
     "y": 759
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1681601844667,
    "source_hash": "6d6695fc"
   },
   "outputs": [],
   "source": [
    "def try_everything(input_sz:   int,\n",
    "                   output_sz:   int,\n",
    "                   funs:       list,\n",
    "                   lims:       list,\n",
    "                   acts:       list,\n",
    "                   max_layers: int,\n",
    "                   n_epochs:   int, \n",
    "                   criterion:  torch.nn.modules.loss, \n",
    "                   data:       tuple):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        - input_sz:   the size of the input layer\n",
    "        - output_sz:  the size of the output layer\n",
    "        - funs:       a list of functions to be used to generate the hidden layer structures\n",
    "        - lims:       a list of the upper limits of the range of the input to each function in funs\n",
    "        - acts:       a list of activation functions to be used\n",
    "        - max_layers: the maximum number of layers allowed in the neural network\n",
    "        - n_epochs:   the number of epochs to train the neural network for\n",
    "        - criterion:  the loss function to be used\n",
    "        - data:       a tuple of three datasets, consisting of training data,\n",
    "                      validation data, and test data\n",
    "        \n",
    "    OUTPUT:\n",
    "        - a tuple of the best score achieved by any of the models tested and the model that achieved \n",
    "          that score\n",
    "    \"\"\"\n",
    "\n",
    "    train_sec, val_sec, test_sec = data\n",
    "\n",
    "    # list of models\n",
    "    models = []\n",
    "\n",
    "    # list of each model score\n",
    "    scores = []  \n",
    "\n",
    "    for act in acts:\n",
    "        for fun, lim in zip(funs, lims):\n",
    "            for i in range(2, max_layers+1):\n",
    "\n",
    "                sample_points = np.linspace(0, lim, i)\n",
    "                structure     = fun(sample_points) \n",
    "\n",
    "                model = NeuralNetwork()\n",
    "                \n",
    "                # Define your layers for the model \n",
    "                model_struc = nn.ModuleList()\n",
    "\n",
    "                # first layer\n",
    "                model_struc.append(nn.Linear(input_sz, structure[0]))\n",
    "\n",
    "                # hidden layers\n",
    "                for i, n in enumerate(structure[:-1]):\n",
    "                    model_struc.append(nn.Linear(structure[i], structure[i+1]))\n",
    "        \n",
    "                # output layer\n",
    "                model_struc.append(nn.Linear(structure[-1], output_sz))\n",
    "                model_struc.append(nn.Linear(3, output_sz))\n",
    "\n",
    "                #store the structure into the model \n",
    "                model.set_model(model_struc)\n",
    "                model.set_activation(act)\n",
    "\n",
    "                #store the model\n",
    "                models.append(model)\n",
    "                \n",
    "    score = -np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for model in models:\n",
    "        optimizer.zero_grad()\n",
    "        train_model(model,\n",
    "                    optimizer,\n",
    "                    criterion,\n",
    "                    train_data,\n",
    "                    val_data,\n",
    "                    n_epochs) \n",
    "\n",
    "        evaluation = test_model_performance(model, test_sec)\n",
    "\n",
    "        if (score < evaluation):\n",
    "            best_model = model\n",
    "            score = evaluation\n",
    "    \n",
    "    return (score, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6daa7d494717408796ec3bc3609ace52",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 821
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Probamos la funci贸n, con dos arquitecturas de redes diferentes y cuatro funciones de activaciones diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "efac7390726440bf919203639f5424bf",
    "deepnote_app_coordinates": {
     "h": 35,
     "w": 12,
     "x": 0,
     "y": 827
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 264414,
    "execution_start": 1681601846372,
    "source_hash": "a92f01d9"
   },
   "outputs": [],
   "source": [
    "score, best_model = try_everything(input_sz = 20,\n",
    "                                   output_sz = 3,\n",
    "                                   funs = [g1, g2, g3],\n",
    "                                   lims = [130, 6, 1],\n",
    "                                   acts = [nn.ReLU(), nn.LeakyReLU(), nn.Sigmoid(), nn.Tanh()],\n",
    "                                   max_layers = 5,\n",
    "                                   n_epochs   = 50, \n",
    "                                   criterion  = nn.CrossEntropyLoss(), \n",
    "                                   data       = (train_sec, val_sec, test_sec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "286b6f4bcf5a4fcdb8dbf594fdc018a0",
    "deepnote_app_coordinates": {
     "h": 6,
     "w": 12,
     "x": 0,
     "y": 879
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Como se puede observar el resultado no es que mejore mucho, pero aumenta algo a diferencia de nuestro primer modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3fdd2d7aee4e4195beadfcd22d037092",
    "deepnote_app_coordinates": {
     "h": 15,
     "w": 12,
     "x": 0,
     "y": 863
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 23,
    "execution_start": 1681602110791,
    "source_hash": "1f8baaa7"
   },
   "outputs": [],
   "source": [
    "print(f\"Nuestro modelo ganador tuvo {score}% de precisi贸n y fue:\\n\")\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a14e40ce214b4744aa2c5b9d94218aeb",
    "deepnote_app_coordinates": {
     "h": 20,
     "w": 12,
     "x": 0,
     "y": 886
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Adem谩s, al elegir de manera aleatoria un registro del dataset y utilizar el modelo para predecir su sentimiento, en la mayor铆a de casos nos est谩 dando la emoci贸n enojado, no entendemos por qu茅 el modelo se cesga a tratar de clasificar los audios como bravos. Nuestra hip贸tesis es que faltan m谩s datos para que el modelo pueda entender mejor.\n",
    "\n",
    "Decimos esto en vista de haber probado todas las opciones que se nos ocurrieron; probamos distintas funciones de activaci贸n, n煤mero distinto de capas, distintas arquitecturas de red neuronal, datos estandarizados y sin estandarizar, sin embargo no fue suficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "aabfd8a24eb841ba9866dcc2343c6930",
    "deepnote_app_coordinates": {
     "h": 13,
     "w": 12,
     "x": 0,
     "y": 907
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1681602110821,
    "source_hash": "532837ea"
   },
   "outputs": [],
   "source": [
    "a = np.random.randint(test_sec.X.shape[0])\n",
    "sample = test_sec.X[a,:]\n",
    "\n",
    "print(sample.reshape((1,-1)))\n",
    "best_model.make_prediction(sample.reshape((1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "bf9c51906f404b36906aed20e4545cc6",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=839e86b6-ffa4-40c1-9d7e-74e8c82a69a5' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_app_layout": "article",
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "ecaa1bfc5b3240848cf819ea82fdf830",
  "deepnote_persisted_session": {
   "createdAt": "2023-04-16T00:01:11.611Z"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
