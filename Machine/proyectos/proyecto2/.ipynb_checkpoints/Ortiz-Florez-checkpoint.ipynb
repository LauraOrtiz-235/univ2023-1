{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7d48f1d98987473687b3599872f47aad",
    "deepnote_app_coordinates": {
     "h": 14,
     "w": 12,
     "x": 0,
     "y": 1
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# ðŸ”Š Proyecto 2  _(detecciÃ³n de emociones por voz)_ ðŸ”Š "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "de07fbdb77a142189107f3ad66505c96",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 16
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "ðŸ’¡IntegrantesðŸ’¡\n",
    "\n",
    "âœ¨Laura SofÃ­a Ortiz Arcos\n",
    "âœ¨David Santiago FlÃ³rez Alsina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "43f0ed3755934f4ea0edfeb9a194e3ad",
    "deepnote_app_coordinates": {
     "h": 16,
     "w": 12,
     "x": 0,
     "y": 22
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5886,
    "execution_start": 1681601811736,
    "source_hash": "8a84a1e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn                 import preprocessing\n",
    "from torch.utils.data        import DataLoader\n",
    "from torch.utils.data        import TensorDataset\n",
    "from sklearn.preprocessing   import StandardScaler, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1c28773db58e467fa0ccd9bc044573b7",
    "deepnote_app_coordinates": {
     "h": 8,
     "w": 12,
     "x": 0,
     "y": 39
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Cargar y preparar los datos\n",
    "\n",
    "Para esta parte primero cargaremos los datos sobre seÃ±ales de voz, de acuerdo al sentimiento de dicha voz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "e86432980a69469191388ed4a4274fad",
    "deepnote_app_coordinates": {
     "h": 18,
     "w": 12,
     "x": 0,
     "y": 48
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 244,
    "execution_start": 1681601817626,
    "source_hash": "d270039a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.deepnote.dataframe.v3+json": {
       "column_count": 24,
       "columns": [
        {
         "dtype": "int64",
         "name": "Unnamed: 0",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.5,
            "bin_start": 0,
            "count": 1
           },
           {
            "bin_end": 1,
            "bin_start": 0.5,
            "count": 0
           },
           {
            "bin_end": 1.5,
            "bin_start": 1,
            "count": 1
           },
           {
            "bin_end": 2,
            "bin_start": 1.5,
            "count": 0
           },
           {
            "bin_end": 2.5,
            "bin_start": 2,
            "count": 1
           },
           {
            "bin_end": 3,
            "bin_start": 2.5,
            "count": 0
           },
           {
            "bin_end": 3.5,
            "bin_start": 3,
            "count": 0
           },
           {
            "bin_end": 4,
            "bin_start": 3.5,
            "count": 0
           },
           {
            "bin_end": 4.5,
            "bin_start": 4,
            "count": 1
           },
           {
            "bin_end": 5,
            "bin_start": 4.5,
            "count": 1
           }
          ],
          "max": "5",
          "min": "0",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "int64",
         "name": "Unnamed: 0.1",
         "stats": {
          "histogram": [
           {
            "bin_end": 1.5,
            "bin_start": 1,
            "count": 1
           },
           {
            "bin_end": 2,
            "bin_start": 1.5,
            "count": 0
           },
           {
            "bin_end": 2.5,
            "bin_start": 2,
            "count": 1
           },
           {
            "bin_end": 3,
            "bin_start": 2.5,
            "count": 0
           },
           {
            "bin_end": 3.5,
            "bin_start": 3,
            "count": 1
           },
           {
            "bin_end": 4,
            "bin_start": 3.5,
            "count": 0
           },
           {
            "bin_end": 4.5,
            "bin_start": 4,
            "count": 0
           },
           {
            "bin_end": 5,
            "bin_start": 4.5,
            "count": 0
           },
           {
            "bin_end": 5.5,
            "bin_start": 5,
            "count": 1
           },
           {
            "bin_end": 6,
            "bin_start": 5.5,
            "count": 1
           }
          ],
          "max": "6",
          "min": "1",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "int64",
         "name": "X",
         "stats": {
          "histogram": [
           {
            "bin_end": 1.5,
            "bin_start": 1,
            "count": 1
           },
           {
            "bin_end": 2,
            "bin_start": 1.5,
            "count": 0
           },
           {
            "bin_end": 2.5,
            "bin_start": 2,
            "count": 1
           },
           {
            "bin_end": 3,
            "bin_start": 2.5,
            "count": 0
           },
           {
            "bin_end": 3.5,
            "bin_start": 3,
            "count": 1
           },
           {
            "bin_end": 4,
            "bin_start": 3.5,
            "count": 0
           },
           {
            "bin_end": 4.5,
            "bin_start": 4,
            "count": 0
           },
           {
            "bin_end": 5,
            "bin_start": 4.5,
            "count": 0
           },
           {
            "bin_end": 5.5,
            "bin_start": 5,
            "count": 1
           },
           {
            "bin_end": 6,
            "bin_start": 5.5,
            "count": 1
           }
          ],
          "max": "6",
          "min": "1",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "meanfreq",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.1708239682,
            "bin_start": 0.16879306,
            "count": 1
           },
           {
            "bin_end": 0.1728548764,
            "bin_start": 0.1708239682,
            "count": 0
           },
           {
            "bin_end": 0.17488578459999998,
            "bin_start": 0.1728548764,
            "count": 0
           },
           {
            "bin_end": 0.1769166928,
            "bin_start": 0.17488578459999998,
            "count": 0
           },
           {
            "bin_end": 0.178947601,
            "bin_start": 0.1769166928,
            "count": 0
           },
           {
            "bin_end": 0.1809785092,
            "bin_start": 0.178947601,
            "count": 0
           },
           {
            "bin_end": 0.18300941739999999,
            "bin_start": 0.1809785092,
            "count": 1
           },
           {
            "bin_end": 0.1850403256,
            "bin_start": 0.18300941739999999,
            "count": 1
           },
           {
            "bin_end": 0.18707123380000001,
            "bin_start": 0.1850403256,
            "count": 1
           },
           {
            "bin_end": 0.189102142,
            "bin_start": 0.18707123380000001,
            "count": 1
           }
          ],
          "max": "0.189102142",
          "min": "0.16879306",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "sd",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.05840932919999999,
            "bin_start": 0.057910224,
            "count": 1
           },
           {
            "bin_end": 0.05890843439999998,
            "bin_start": 0.05840932919999999,
            "count": 0
           },
           {
            "bin_end": 0.05940753959999997,
            "bin_start": 0.05890843439999998,
            "count": 0
           },
           {
            "bin_end": 0.05990664479999996,
            "bin_start": 0.05940753959999997,
            "count": 0
           },
           {
            "bin_end": 0.06040574999999995,
            "bin_start": 0.05990664479999996,
            "count": 1
           },
           {
            "bin_end": 0.06090485519999994,
            "bin_start": 0.06040574999999995,
            "count": 1
           },
           {
            "bin_end": 0.06140396039999993,
            "bin_start": 0.06090485519999994,
            "count": 0
           },
           {
            "bin_end": 0.06190306559999992,
            "bin_start": 0.06140396039999993,
            "count": 0
           },
           {
            "bin_end": 0.062402170799999906,
            "bin_start": 0.06190306559999992,
            "count": 1
           },
           {
            "bin_end": 0.0629012759999999,
            "bin_start": 0.062402170799999906,
            "count": 1
           }
          ],
          "max": "0.0629012759999999",
          "min": "0.057910224",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "median",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.1611340528,
            "bin_start": 0.156266137,
            "count": 1
           },
           {
            "bin_end": 0.1660019686,
            "bin_start": 0.1611340528,
            "count": 0
           },
           {
            "bin_end": 0.1708698844,
            "bin_start": 0.1660019686,
            "count": 0
           },
           {
            "bin_end": 0.1757378002,
            "bin_start": 0.1708698844,
            "count": 1
           },
           {
            "bin_end": 0.180605716,
            "bin_start": 0.1757378002,
            "count": 0
           },
           {
            "bin_end": 0.18547363179999998,
            "bin_start": 0.180605716,
            "count": 0
           },
           {
            "bin_end": 0.1903415476,
            "bin_start": 0.18547363179999998,
            "count": 1
           },
           {
            "bin_end": 0.1952094634,
            "bin_start": 0.1903415476,
            "count": 1
           },
           {
            "bin_end": 0.2000773792,
            "bin_start": 0.1952094634,
            "count": 0
           },
           {
            "bin_end": 0.204945295,
            "bin_start": 0.2000773792,
            "count": 1
           }
          ],
          "max": "0.204945295",
          "min": "0.156266137",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "Q25",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.11824650169999991,
            "bin_start": 0.1167825219999999,
            "count": 1
           },
           {
            "bin_end": 0.11971048139999993,
            "bin_start": 0.11824650169999991,
            "count": 0
           },
           {
            "bin_end": 0.12117446109999994,
            "bin_start": 0.11971048139999993,
            "count": 0
           },
           {
            "bin_end": 0.12263844079999994,
            "bin_start": 0.12117446109999994,
            "count": 0
           },
           {
            "bin_end": 0.12410242049999995,
            "bin_start": 0.12263844079999994,
            "count": 0
           },
           {
            "bin_end": 0.12556640019999996,
            "bin_start": 0.12410242049999995,
            "count": 0
           },
           {
            "bin_end": 0.1270303799,
            "bin_start": 0.12556640019999996,
            "count": 1
           },
           {
            "bin_end": 0.1284943596,
            "bin_start": 0.1270303799,
            "count": 0
           },
           {
            "bin_end": 0.1299583393,
            "bin_start": 0.1284943596,
            "count": 1
           },
           {
            "bin_end": 0.131422319,
            "bin_start": 0.1299583393,
            "count": 2
           }
          ],
          "max": "0.131422319",
          "min": "0.1167825219999999",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "Q75",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.2196909598,
            "bin_start": 0.21632572,
            "count": 1
           },
           {
            "bin_end": 0.2230561996,
            "bin_start": 0.2196909598,
            "count": 0
           },
           {
            "bin_end": 0.2264214394,
            "bin_start": 0.2230561996,
            "count": 0
           },
           {
            "bin_end": 0.2297866792,
            "bin_start": 0.2264214394,
            "count": 0
           },
           {
            "bin_end": 0.23315191899999999,
            "bin_start": 0.2297866792,
            "count": 0
           },
           {
            "bin_end": 0.23651715880000002,
            "bin_start": 0.23315191899999999,
            "count": 1
           },
           {
            "bin_end": 0.2398823986,
            "bin_start": 0.23651715880000002,
            "count": 1
           },
           {
            "bin_end": 0.2432476384,
            "bin_start": 0.2398823986,
            "count": 0
           },
           {
            "bin_end": 0.2466128782,
            "bin_start": 0.2432476384,
            "count": 1
           },
           {
            "bin_end": 0.249978118,
            "bin_start": 0.2466128782,
            "count": 1
           }
          ],
          "max": "0.249978118",
          "min": "0.21632572",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "IQR",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.1014444581,
            "bin_start": 0.099543198,
            "count": 1
           },
           {
            "bin_end": 0.1033457182,
            "bin_start": 0.1014444581,
            "count": 0
           },
           {
            "bin_end": 0.1052469783,
            "bin_start": 0.1033457182,
            "count": 0
           },
           {
            "bin_end": 0.1071482384,
            "bin_start": 0.1052469783,
            "count": 1
           },
           {
            "bin_end": 0.10904949850000001,
            "bin_start": 0.1071482384,
            "count": 1
           },
           {
            "bin_end": 0.1109507586,
            "bin_start": 0.10904949850000001,
            "count": 0
           },
           {
            "bin_end": 0.1128520187,
            "bin_start": 0.1109507586,
            "count": 0
           },
           {
            "bin_end": 0.1147532788,
            "bin_start": 0.1128520187,
            "count": 1
           },
           {
            "bin_end": 0.11665453890000001,
            "bin_start": 0.1147532788,
            "count": 0
           },
           {
            "bin_end": 0.118555799,
            "bin_start": 0.11665453890000001,
            "count": 1
           }
          ],
          "max": "0.118555799",
          "min": "0.099543198",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "skew",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.9208626947,
            "bin_start": 0.8690878000000001,
            "count": 1
           },
           {
            "bin_end": 0.9726375894,
            "bin_start": 0.9208626947,
            "count": 0
           },
           {
            "bin_end": 1.0244124841,
            "bin_start": 0.9726375894,
            "count": 0
           },
           {
            "bin_end": 1.0761873788,
            "bin_start": 1.0244124841,
            "count": 0
           },
           {
            "bin_end": 1.1279622735,
            "bin_start": 1.0761873788,
            "count": 1
           },
           {
            "bin_end": 1.1797371682,
            "bin_start": 1.1279622735,
            "count": 0
           },
           {
            "bin_end": 1.2315120628999998,
            "bin_start": 1.1797371682,
            "count": 1
           },
           {
            "bin_end": 1.2832869576,
            "bin_start": 1.2315120628999998,
            "count": 0
           },
           {
            "bin_end": 1.3350618523,
            "bin_start": 1.2832869576,
            "count": 1
           },
           {
            "bin_end": 1.3868367469999998,
            "bin_start": 1.3350618523,
            "count": 1
           }
          ],
          "max": "1.3868367469999998",
          "min": "0.8690878000000001",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "kurt",
         "stats": {
          "histogram": [
           {
            "bin_end": 3.080519593,
            "bin_start": 2.863716892,
            "count": 1
           },
           {
            "bin_end": 3.2973222939999998,
            "bin_start": 3.080519593,
            "count": 0
           },
           {
            "bin_end": 3.5141249949999995,
            "bin_start": 3.2973222939999998,
            "count": 0
           },
           {
            "bin_end": 3.7309276959999997,
            "bin_start": 3.5141249949999995,
            "count": 1
           },
           {
            "bin_end": 3.947730397,
            "bin_start": 3.7309276959999997,
            "count": 1
           },
           {
            "bin_end": 4.164533098,
            "bin_start": 3.947730397,
            "count": 0
           },
           {
            "bin_end": 4.3813357989999995,
            "bin_start": 4.164533098,
            "count": 0
           },
           {
            "bin_end": 4.598138499999999,
            "bin_start": 4.3813357989999995,
            "count": 1
           },
           {
            "bin_end": 4.814941201,
            "bin_start": 4.598138499999999,
            "count": 0
           },
           {
            "bin_end": 5.031743902,
            "bin_start": 4.814941201,
            "count": 1
           }
          ],
          "max": "5.031743902",
          "min": "2.863716892",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "sp.ent",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.9195866448,
            "bin_start": 0.91884764,
            "count": 2
           },
           {
            "bin_end": 0.9203256496,
            "bin_start": 0.9195866448,
            "count": 0
           },
           {
            "bin_end": 0.9210646544,
            "bin_start": 0.9203256496,
            "count": 0
           },
           {
            "bin_end": 0.9218036592,
            "bin_start": 0.9210646544,
            "count": 1
           },
           {
            "bin_end": 0.922542664,
            "bin_start": 0.9218036592,
            "count": 0
           },
           {
            "bin_end": 0.9232816688,
            "bin_start": 0.922542664,
            "count": 0
           },
           {
            "bin_end": 0.9240206736,
            "bin_start": 0.9232816688,
            "count": 1
           },
           {
            "bin_end": 0.9247596784,
            "bin_start": 0.9240206736,
            "count": 0
           },
           {
            "bin_end": 0.9254986832,
            "bin_start": 0.9247596784,
            "count": 0
           },
           {
            "bin_end": 0.926237688,
            "bin_start": 0.9254986832,
            "count": 1
           }
          ],
          "max": "0.926237688",
          "min": "0.91884764",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "sfm",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.3026780697,
            "bin_start": 0.298859325,
            "count": 1
           },
           {
            "bin_end": 0.3064968144,
            "bin_start": 0.3026780697,
            "count": 0
           },
           {
            "bin_end": 0.3103155591,
            "bin_start": 0.3064968144,
            "count": 1
           },
           {
            "bin_end": 0.3141343038,
            "bin_start": 0.3103155591,
            "count": 1
           },
           {
            "bin_end": 0.3179530485,
            "bin_start": 0.3141343038,
            "count": 0
           },
           {
            "bin_end": 0.3217717932,
            "bin_start": 0.3179530485,
            "count": 0
           },
           {
            "bin_end": 0.3255905379,
            "bin_start": 0.3217717932,
            "count": 0
           },
           {
            "bin_end": 0.3294092826,
            "bin_start": 0.3255905379,
            "count": 1
           },
           {
            "bin_end": 0.3332280273,
            "bin_start": 0.3294092826,
            "count": 0
           },
           {
            "bin_end": 0.337046772,
            "bin_start": 0.3332280273,
            "count": 1
           }
          ],
          "max": "0.337046772",
          "min": "0.298859325",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "mode",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.1484906036,
            "bin_start": 0.135648446,
            "count": 1
           },
           {
            "bin_end": 0.1613327612,
            "bin_start": 0.1484906036,
            "count": 2
           },
           {
            "bin_end": 0.17417491880000002,
            "bin_start": 0.1613327612,
            "count": 0
           },
           {
            "bin_end": 0.1870170764,
            "bin_start": 0.17417491880000002,
            "count": 0
           },
           {
            "bin_end": 0.199859234,
            "bin_start": 0.1870170764,
            "count": 0
           },
           {
            "bin_end": 0.2127013916,
            "bin_start": 0.199859234,
            "count": 0
           },
           {
            "bin_end": 0.22554354920000003,
            "bin_start": 0.2127013916,
            "count": 1
           },
           {
            "bin_end": 0.2383857068,
            "bin_start": 0.22554354920000003,
            "count": 0
           },
           {
            "bin_end": 0.2512278644,
            "bin_start": 0.2383857068,
            "count": 0
           },
           {
            "bin_end": 0.264070022,
            "bin_start": 0.2512278644,
            "count": 1
           }
          ],
          "max": "0.264070022",
          "min": "0.135648446",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "centroid",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.1708239682,
            "bin_start": 0.16879306,
            "count": 1
           },
           {
            "bin_end": 0.1728548764,
            "bin_start": 0.1708239682,
            "count": 0
           },
           {
            "bin_end": 0.17488578459999998,
            "bin_start": 0.1728548764,
            "count": 0
           },
           {
            "bin_end": 0.1769166928,
            "bin_start": 0.17488578459999998,
            "count": 0
           },
           {
            "bin_end": 0.178947601,
            "bin_start": 0.1769166928,
            "count": 0
           },
           {
            "bin_end": 0.1809785092,
            "bin_start": 0.178947601,
            "count": 0
           },
           {
            "bin_end": 0.18300941739999999,
            "bin_start": 0.1809785092,
            "count": 1
           },
           {
            "bin_end": 0.1850403256,
            "bin_start": 0.18300941739999999,
            "count": 1
           },
           {
            "bin_end": 0.18707123380000001,
            "bin_start": 0.1850403256,
            "count": 1
           },
           {
            "bin_end": 0.189102142,
            "bin_start": 0.18707123380000001,
            "count": 1
           }
          ],
          "max": "0.189102142",
          "min": "0.16879306",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "meanfun",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.1125221278,
            "bin_start": 0.109719895,
            "count": 1
           },
           {
            "bin_end": 0.1153243606,
            "bin_start": 0.1125221278,
            "count": 0
           },
           {
            "bin_end": 0.1181265934,
            "bin_start": 0.1153243606,
            "count": 0
           },
           {
            "bin_end": 0.12092882619999999,
            "bin_start": 0.1181265934,
            "count": 0
           },
           {
            "bin_end": 0.123731059,
            "bin_start": 0.12092882619999999,
            "count": 1
           },
           {
            "bin_end": 0.1265332918,
            "bin_start": 0.123731059,
            "count": 1
           },
           {
            "bin_end": 0.1293355246,
            "bin_start": 0.1265332918,
            "count": 1
           },
           {
            "bin_end": 0.1321377574,
            "bin_start": 0.1293355246,
            "count": 0
           },
           {
            "bin_end": 0.1349399902,
            "bin_start": 0.1321377574,
            "count": 0
           },
           {
            "bin_end": 0.137742223,
            "bin_start": 0.1349399902,
            "count": 1
           }
          ],
          "max": "0.137742223",
          "min": "0.109719895",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "minfun",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.0249041045,
            "bin_start": 0.018411968,
            "count": 3
           },
           {
            "bin_end": 0.031396241,
            "bin_start": 0.0249041045,
            "count": 0
           },
           {
            "bin_end": 0.0378883775,
            "bin_start": 0.031396241,
            "count": 0
           },
           {
            "bin_end": 0.044380513999999996,
            "bin_start": 0.0378883775,
            "count": 0
           },
           {
            "bin_end": 0.0508726505,
            "bin_start": 0.044380513999999996,
            "count": 1
           },
           {
            "bin_end": 0.057364787,
            "bin_start": 0.0508726505,
            "count": 0
           },
           {
            "bin_end": 0.0638569235,
            "bin_start": 0.057364787,
            "count": 0
           },
           {
            "bin_end": 0.07034905999999999,
            "bin_start": 0.0638569235,
            "count": 0
           },
           {
            "bin_end": 0.0768411965,
            "bin_start": 0.07034905999999999,
            "count": 0
           },
           {
            "bin_end": 0.083333333,
            "bin_start": 0.0768411965,
            "count": 1
           }
          ],
          "max": "0.083333333",
          "min": "0.018411968",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "maxfun",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.2388833503,
            "bin_start": 0.235294118,
            "count": 1
           },
           {
            "bin_end": 0.2424725826,
            "bin_start": 0.2388833503,
            "count": 0
           },
           {
            "bin_end": 0.2460618149,
            "bin_start": 0.2424725826,
            "count": 0
           },
           {
            "bin_end": 0.2496510472,
            "bin_start": 0.2460618149,
            "count": 0
           },
           {
            "bin_end": 0.2532402795,
            "bin_start": 0.2496510472,
            "count": 0
           },
           {
            "bin_end": 0.2568295118,
            "bin_start": 0.2532402795,
            "count": 0
           },
           {
            "bin_end": 0.2604187441,
            "bin_start": 0.2568295118,
            "count": 1
           },
           {
            "bin_end": 0.2640079764,
            "bin_start": 0.2604187441,
            "count": 1
           },
           {
            "bin_end": 0.2675972087,
            "bin_start": 0.2640079764,
            "count": 0
           },
           {
            "bin_end": 0.271186441,
            "bin_start": 0.2675972087,
            "count": 2
           }
          ],
          "max": "0.271186441",
          "min": "0.235294118",
          "nan_count": 0,
          "unique_count": 4
         }
        },
        {
         "dtype": "float64",
         "name": "meandom",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.3071171018,
            "bin_start": 0.228794643,
            "count": 1
           },
           {
            "bin_end": 0.3854395606,
            "bin_start": 0.3071171018,
            "count": 1
           },
           {
            "bin_end": 0.46376201940000006,
            "bin_start": 0.3854395606,
            "count": 0
           },
           {
            "bin_end": 0.5420844782,
            "bin_start": 0.46376201940000006,
            "count": 0
           },
           {
            "bin_end": 0.620406937,
            "bin_start": 0.5420844782,
            "count": 0
           },
           {
            "bin_end": 0.6987293958,
            "bin_start": 0.620406937,
            "count": 0
           },
           {
            "bin_end": 0.7770518546,
            "bin_start": 0.6987293958,
            "count": 0
           },
           {
            "bin_end": 0.8553743134,
            "bin_start": 0.7770518546,
            "count": 1
           },
           {
            "bin_end": 0.9336967722,
            "bin_start": 0.8553743134,
            "count": 1
           },
           {
            "bin_end": 1.012019231,
            "bin_start": 0.9336967722,
            "count": 1
           }
          ],
          "max": "1.012019231",
          "min": "0.228794643",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "mindom",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.08671875,
            "bin_start": 0.0859375,
            "count": 4
           },
           {
            "bin_end": 0.0875,
            "bin_start": 0.08671875,
            "count": 0
           },
           {
            "bin_end": 0.08828125,
            "bin_start": 0.0875,
            "count": 0
           },
           {
            "bin_end": 0.0890625,
            "bin_start": 0.08828125,
            "count": 0
           },
           {
            "bin_end": 0.08984375,
            "bin_start": 0.0890625,
            "count": 0
           },
           {
            "bin_end": 0.090625,
            "bin_start": 0.08984375,
            "count": 0
           },
           {
            "bin_end": 0.09140625,
            "bin_start": 0.090625,
            "count": 0
           },
           {
            "bin_end": 0.0921875,
            "bin_start": 0.09140625,
            "count": 0
           },
           {
            "bin_end": 0.09296875,
            "bin_start": 0.0921875,
            "count": 0
           },
           {
            "bin_end": 0.09375,
            "bin_start": 0.09296875,
            "count": 1
           }
          ],
          "max": "0.09375",
          "min": "0.0859375",
          "nan_count": 0,
          "unique_count": 2
         }
        },
        {
         "dtype": "float64",
         "name": "maxdom",
         "stats": {
          "histogram": [
           {
            "bin_end": 1.18515625,
            "bin_start": 0.625,
            "count": 2
           },
           {
            "bin_end": 1.7453125,
            "bin_start": 1.18515625,
            "count": 0
           },
           {
            "bin_end": 2.30546875,
            "bin_start": 1.7453125,
            "count": 0
           },
           {
            "bin_end": 2.865625,
            "bin_start": 2.30546875,
            "count": 0
           },
           {
            "bin_end": 3.42578125,
            "bin_start": 2.865625,
            "count": 0
           },
           {
            "bin_end": 3.9859375000000004,
            "bin_start": 3.42578125,
            "count": 0
           },
           {
            "bin_end": 4.546093750000001,
            "bin_start": 3.9859375000000004,
            "count": 1
           },
           {
            "bin_end": 5.10625,
            "bin_start": 4.546093750000001,
            "count": 0
           },
           {
            "bin_end": 5.6664062500000005,
            "bin_start": 5.10625,
            "count": 1
           },
           {
            "bin_end": 6.2265625,
            "bin_start": 5.6664062500000005,
            "count": 1
           }
          ],
          "max": "6.2265625",
          "min": "0.625",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "dfrange",
         "stats": {
          "histogram": [
           {
            "bin_end": 1.09921875,
            "bin_start": 0.5390625,
            "count": 2
           },
           {
            "bin_end": 1.659375,
            "bin_start": 1.09921875,
            "count": 0
           },
           {
            "bin_end": 2.21953125,
            "bin_start": 1.659375,
            "count": 0
           },
           {
            "bin_end": 2.7796875,
            "bin_start": 2.21953125,
            "count": 0
           },
           {
            "bin_end": 3.33984375,
            "bin_start": 2.7796875,
            "count": 0
           },
           {
            "bin_end": 3.9000000000000004,
            "bin_start": 3.33984375,
            "count": 0
           },
           {
            "bin_end": 4.460156250000001,
            "bin_start": 3.9000000000000004,
            "count": 1
           },
           {
            "bin_end": 5.0203125,
            "bin_start": 4.460156250000001,
            "count": 0
           },
           {
            "bin_end": 5.5804687500000005,
            "bin_start": 5.0203125,
            "count": 1
           },
           {
            "bin_end": 6.140625,
            "bin_start": 5.5804687500000005,
            "count": 1
           }
          ],
          "max": "6.140625",
          "min": "0.5390625",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "float64",
         "name": "modindx",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.1384053945,
            "bin_start": 0.116585704,
            "count": 1
           },
           {
            "bin_end": 0.160225085,
            "bin_start": 0.1384053945,
            "count": 1
           },
           {
            "bin_end": 0.18204477549999998,
            "bin_start": 0.160225085,
            "count": 0
           },
           {
            "bin_end": 0.203864466,
            "bin_start": 0.18204477549999998,
            "count": 0
           },
           {
            "bin_end": 0.2256841565,
            "bin_start": 0.203864466,
            "count": 0
           },
           {
            "bin_end": 0.247503847,
            "bin_start": 0.2256841565,
            "count": 0
           },
           {
            "bin_end": 0.2693235375,
            "bin_start": 0.247503847,
            "count": 0
           },
           {
            "bin_end": 0.291143228,
            "bin_start": 0.2693235375,
            "count": 0
           },
           {
            "bin_end": 0.31296291849999996,
            "bin_start": 0.291143228,
            "count": 2
           },
           {
            "bin_end": 0.334782609,
            "bin_start": 0.31296291849999996,
            "count": 1
           }
          ],
          "max": "0.334782609",
          "min": "0.116585704",
          "nan_count": 0,
          "unique_count": 5
         }
        },
        {
         "dtype": "object",
         "name": "label",
         "stats": {
          "categories": [
           {
            "count": 5,
            "name": "sad"
           }
          ],
          "nan_count": 0,
          "unique_count": 1
         }
        },
        {
         "dtype": "int64",
         "name": "_deepnote_index_column"
        }
       ],
       "row_count": 5,
       "rows": [
        {
         "IQR": "0.107388949",
         "Q25": "0.126197183",
         "Q75": "0.233586132",
         "Unnamed: 0": "0",
         "Unnamed: 0.1": "1",
         "X": "1",
         "_deepnote_index_column": "0",
         "centroid": "0.181338105",
         "dfrange": "6.140625",
         "kurt": "2.863716892",
         "label": "sad",
         "maxdom": "6.2265625",
         "maxfun": "0.271186441",
         "meandom": "0.77734375",
         "meanfreq": "0.181338105",
         "meanfun": "0.137742223",
         "median": "0.187475623",
         "mindom": "0.0859375",
         "minfun": "0.0230215829999999",
         "mode": "0.216901408",
         "modindx": "0.116585704",
         "sd": "0.0604951159999999",
         "sfm": "0.307219558",
         "skew": "0.8690878000000001",
         "sp.ent": "0.923566258"
        },
        {
         "IQR": "0.113140407",
         "Q25": "0.130846731",
         "Q75": "0.243987138",
         "Unnamed: 0": "1",
         "Unnamed: 0.1": "2",
         "X": "2",
         "_deepnote_index_column": "1",
         "centroid": "0.186897392",
         "dfrange": "3.9140625",
         "kurt": "3.878650362",
         "label": "sad",
         "maxdom": "4.0",
         "maxfun": "0.271186441",
         "meandom": "0.930338542",
         "meanfreq": "0.186897392",
         "meanfun": "0.121811357",
         "median": "0.195069668",
         "mindom": "0.0859375",
         "minfun": "0.018411968",
         "mode": "0.135648446",
         "modindx": "0.144982762",
         "sd": "0.062260334",
         "sfm": "0.298859325",
         "skew": "1.191767427",
         "sp.ent": "0.91884764"
        },
        {
         "IQR": "0.118555799",
         "Q25": "0.131422319",
         "Q75": "0.249978118",
         "Unnamed: 0": "2",
         "Unnamed: 0.1": "3",
         "X": "3",
         "_deepnote_index_column": "2",
         "centroid": "0.189102142",
         "dfrange": "0.5390625",
         "kurt": "4.5899950480000005",
         "label": "sad",
         "maxdom": "0.625",
         "maxfun": "0.262295082",
         "meandom": "0.332386364",
         "meanfreq": "0.189102142",
         "meanfun": "0.123757895",
         "median": "0.204945295",
         "mindom": "0.0859375",
         "minfun": "0.083333333",
         "mode": "0.264070022",
         "modindx": "0.334782609",
         "sd": "0.0629012759999999",
         "sfm": "0.313068592",
         "skew": "1.312690276",
         "sp.ent": "0.91951906"
        },
        {
         "IQR": "0.107017189",
         "Q25": "0.129949444",
         "Q75": "0.236966633",
         "Unnamed: 0": "4",
         "Unnamed: 0.1": "5",
         "X": "5",
         "_deepnote_index_column": "3",
         "centroid": "0.183036061",
         "dfrange": "5.3828125",
         "kurt": "3.680995381",
         "label": "sad",
         "maxdom": "5.46875",
         "maxfun": "0.258064516",
         "meandom": "1.012019231",
         "meanfreq": "0.183036061",
         "meanfun": "0.128469334",
         "median": "0.174115268",
         "mindom": "0.0859375",
         "minfun": "0.044692737",
         "mode": "0.152032356",
         "modindx": "0.304910498",
         "sd": "0.060051263",
         "sfm": "0.329295377",
         "skew": "1.096408695",
         "sp.ent": "0.921361417"
        },
        {
         "IQR": "0.099543198",
         "Q25": "0.1167825219999999",
         "Q75": "0.21632572",
         "Unnamed: 0": "5",
         "Unnamed: 0.1": "6",
         "X": "6",
         "_deepnote_index_column": "4",
         "centroid": "0.16879306",
         "dfrange": "0.65625",
         "kurt": "5.031743902",
         "label": "sad",
         "maxdom": "0.75",
         "maxfun": "0.235294118",
         "meandom": "0.228794643",
         "meanfreq": "0.16879306",
         "meanfun": "0.109719895",
         "median": "0.156266137",
         "mindom": "0.09375",
         "minfun": "0.02247191",
         "mode": "0.153763654",
         "modindx": "0.306776557",
         "sd": "0.057910224",
         "sfm": "0.337046772",
         "skew": "1.3868367469999998",
         "sp.ent": "0.926237688"
        }
       ]
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>X</th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.187476</td>\n",
       "      <td>0.126197</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.869088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.137742</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>6.226562</td>\n",
       "      <td>6.140625</td>\n",
       "      <td>0.116586</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.195070</td>\n",
       "      <td>0.130847</td>\n",
       "      <td>0.243987</td>\n",
       "      <td>0.113140</td>\n",
       "      <td>1.191767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.121811</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.930339</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.914062</td>\n",
       "      <td>0.144983</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.062901</td>\n",
       "      <td>0.204945</td>\n",
       "      <td>0.131422</td>\n",
       "      <td>0.249978</td>\n",
       "      <td>0.118556</td>\n",
       "      <td>1.312690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.123758</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.332386</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.174115</td>\n",
       "      <td>0.129949</td>\n",
       "      <td>0.236967</td>\n",
       "      <td>0.107017</td>\n",
       "      <td>1.096409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.128469</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>1.012019</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>5.468750</td>\n",
       "      <td>5.382812</td>\n",
       "      <td>0.304910</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.156266</td>\n",
       "      <td>0.116783</td>\n",
       "      <td>0.216326</td>\n",
       "      <td>0.099543</td>\n",
       "      <td>1.386837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.109720</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.228795</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.306777</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  X  meanfreq        sd    median       Q25  \\\n",
       "0           0             1  1  0.181338  0.060495  0.187476  0.126197   \n",
       "1           1             2  2  0.186897  0.062260  0.195070  0.130847   \n",
       "2           2             3  3  0.189102  0.062901  0.204945  0.131422   \n",
       "3           4             5  5  0.183036  0.060051  0.174115  0.129949   \n",
       "4           5             6  6  0.168793  0.057910  0.156266  0.116783   \n",
       "\n",
       "        Q75       IQR      skew  ...  centroid   meanfun    minfun    maxfun  \\\n",
       "0  0.233586  0.107389  0.869088  ...  0.181338  0.137742  0.023022  0.271186   \n",
       "1  0.243987  0.113140  1.191767  ...  0.186897  0.121811  0.018412  0.271186   \n",
       "2  0.249978  0.118556  1.312690  ...  0.189102  0.123758  0.083333  0.262295   \n",
       "3  0.236967  0.107017  1.096409  ...  0.183036  0.128469  0.044693  0.258065   \n",
       "4  0.216326  0.099543  1.386837  ...  0.168793  0.109720  0.022472  0.235294   \n",
       "\n",
       "    meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.777344  0.085938  6.226562  6.140625  0.116586    sad  \n",
       "1  0.930339  0.085938  4.000000  3.914062  0.144983    sad  \n",
       "2  0.332386  0.085938  0.625000  0.539062  0.334783    sad  \n",
       "3  1.012019  0.085938  5.468750  5.382812  0.304910    sad  \n",
       "4  0.228795  0.093750  0.750000  0.656250  0.306777    sad  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('emotions_by_voice_registers.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bcd0c192fe6e408f87b87b0edf557aa3",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 67
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "_**Nos damos algo de informaciÃ³n sobre de quÃ© trata nuestro dataset y cÃ³mo es**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "cdd4e2cb41a84332b050aa71afc6ad61",
    "deepnote_app_coordinates": {
     "h": 27,
     "w": 12,
     "x": 0,
     "y": 72
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16,
    "execution_start": 1681601817903,
    "source_hash": "de1e323c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 909 entries, 0 to 908\n",
      "Data columns (total 24 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Unnamed: 0    909 non-null    int64  \n",
      " 1   Unnamed: 0.1  909 non-null    int64  \n",
      " 2   X             909 non-null    int64  \n",
      " 3   meanfreq      909 non-null    float64\n",
      " 4   sd            909 non-null    float64\n",
      " 5   median        909 non-null    float64\n",
      " 6   Q25           909 non-null    float64\n",
      " 7   Q75           909 non-null    float64\n",
      " 8   IQR           909 non-null    float64\n",
      " 9   skew          909 non-null    float64\n",
      " 10  kurt          909 non-null    float64\n",
      " 11  sp.ent        909 non-null    float64\n",
      " 12  sfm           909 non-null    float64\n",
      " 13  mode          909 non-null    float64\n",
      " 14  centroid      909 non-null    float64\n",
      " 15  meanfun       909 non-null    float64\n",
      " 16  minfun        909 non-null    float64\n",
      " 17  maxfun        909 non-null    float64\n",
      " 18  meandom       909 non-null    float64\n",
      " 19  mindom        909 non-null    float64\n",
      " 20  maxdom        909 non-null    float64\n",
      " 21  dfrange       909 non-null    float64\n",
      " 22  modindx       909 non-null    float64\n",
      " 23  label         909 non-null    object \n",
      "dtypes: float64(20), int64(3), object(1)\n",
      "memory usage: 170.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "82e55d3da4534ee6a761f0ceefcd639c",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 100
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "_**Luego eliminamos los atributos tipo Ã­ndice, en este caso son:**_ 'Unnamed 0', 'Unnamed 0.1' y 'X'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "b8da34bf4d364418bce74e4e4ae19c48",
    "deepnote_app_coordinates": {
     "h": 26,
     "w": 12,
     "x": 0,
     "y": 105
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 122,
    "execution_start": 1681601817963,
    "source_hash": "1f73118f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.deepnote.dataframe.v3+json": {
       "column_count": 21,
       "columns": [
        {
         "dtype": "float64",
         "name": "meanfreq",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.1500290333,
            "bin_start": 0.13889185,
            "count": 4
           },
           {
            "bin_end": 0.16116621660000002,
            "bin_start": 0.1500290333,
            "count": 11
           },
           {
            "bin_end": 0.17230339990000001,
            "bin_start": 0.16116621660000002,
            "count": 34
           },
           {
            "bin_end": 0.1834405832,
            "bin_start": 0.17230339990000001,
            "count": 137
           },
           {
            "bin_end": 0.19457776650000003,
            "bin_start": 0.1834405832,
            "count": 141
           },
           {
            "bin_end": 0.20571494980000002,
            "bin_start": 0.19457776650000003,
            "count": 113
           },
           {
            "bin_end": 0.2168521331,
            "bin_start": 0.20571494980000002,
            "count": 131
           },
           {
            "bin_end": 0.22798931640000003,
            "bin_start": 0.2168521331,
            "count": 136
           },
           {
            "bin_end": 0.23912649970000002,
            "bin_start": 0.22798931640000003,
            "count": 148
           },
           {
            "bin_end": 0.250263683,
            "bin_start": 0.23912649970000002,
            "count": 54
           }
          ],
          "max": "0.250263683",
          "min": "0.13889185",
          "nan_count": 0,
          "unique_count": 909
         }
        },
        {
         "dtype": "float64",
         "name": "sd",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.03172653199999999,
            "bin_start": 0.024114037,
            "count": 38
           },
           {
            "bin_end": 0.039339026999999985,
            "bin_start": 0.03172653199999999,
            "count": 229
           },
           {
            "bin_end": 0.04695152199999997,
            "bin_start": 0.039339026999999985,
            "count": 156
           },
           {
            "bin_end": 0.054564016999999965,
            "bin_start": 0.04695152199999997,
            "count": 92
           },
           {
            "bin_end": 0.06217651199999995,
            "bin_start": 0.054564016999999965,
            "count": 153
           },
           {
            "bin_end": 0.06978900699999994,
            "bin_start": 0.06217651199999995,
            "count": 149
           },
           {
            "bin_end": 0.07740150199999993,
            "bin_start": 0.06978900699999994,
            "count": 47
           },
           {
            "bin_end": 0.08501399699999992,
            "bin_start": 0.07740150199999993,
            "count": 24
           },
           {
            "bin_end": 0.09262649199999992,
            "bin_start": 0.08501399699999992,
            "count": 14
           },
           {
            "bin_end": 0.1002389869999999,
            "bin_start": 0.09262649199999992,
            "count": 7
           }
          ],
          "max": "0.1002389869999999",
          "min": "0.024114037",
          "nan_count": 0,
          "unique_count": 909
         }
        },
        {
         "dtype": "float64",
         "name": "median",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.1519732112,
            "bin_start": 0.139471698,
            "count": 5
           },
           {
            "bin_end": 0.1644747244,
            "bin_start": 0.1519732112,
            "count": 35
           },
           {
            "bin_end": 0.1769762376,
            "bin_start": 0.1644747244,
            "count": 92
           },
           {
            "bin_end": 0.1894777508,
            "bin_start": 0.1769762376,
            "count": 72
           },
           {
            "bin_end": 0.201979264,
            "bin_start": 0.1894777508,
            "count": 113
           },
           {
            "bin_end": 0.2144807772,
            "bin_start": 0.201979264,
            "count": 115
           },
           {
            "bin_end": 0.2269822904,
            "bin_start": 0.2144807772,
            "count": 143
           },
           {
            "bin_end": 0.2394838036,
            "bin_start": 0.2269822904,
            "count": 189
           },
           {
            "bin_end": 0.2519853168,
            "bin_start": 0.2394838036,
            "count": 87
           },
           {
            "bin_end": 0.26448683,
            "bin_start": 0.2519853168,
            "count": 58
           }
          ],
          "max": "0.26448683",
          "min": "0.139471698",
          "nan_count": 0,
          "unique_count": 871
         }
        },
        {
         "dtype": "float64",
         "name": "Q25",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.0665454543999999,
            "bin_start": 0.0461111109999999,
            "count": 2
           },
           {
            "bin_end": 0.08697979779999991,
            "bin_start": 0.0665454543999999,
            "count": 1
           },
           {
            "bin_end": 0.10741414119999992,
            "bin_start": 0.08697979779999991,
            "count": 23
           },
           {
            "bin_end": 0.12784848459999992,
            "bin_start": 0.10741414119999992,
            "count": 79
           },
           {
            "bin_end": 0.14828282799999992,
            "bin_start": 0.12784848459999992,
            "count": 139
           },
           {
            "bin_end": 0.16871717139999992,
            "bin_start": 0.14828282799999992,
            "count": 114
           },
           {
            "bin_end": 0.18915151479999995,
            "bin_start": 0.16871717139999992,
            "count": 135
           },
           {
            "bin_end": 0.20958585819999995,
            "bin_start": 0.18915151479999995,
            "count": 164
           },
           {
            "bin_end": 0.23002020159999995,
            "bin_start": 0.20958585819999995,
            "count": 221
           },
           {
            "bin_end": 0.250454545,
            "bin_start": 0.23002020159999995,
            "count": 31
           }
          ],
          "max": "0.250454545",
          "min": "0.0461111109999999",
          "nan_count": 0,
          "unique_count": 884
         }
        },
        {
         "dtype": "float64",
         "name": "Q75",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.2014351392,
            "bin_start": 0.193367037,
            "count": 7
           },
           {
            "bin_end": 0.20950324139999998,
            "bin_start": 0.2014351392,
            "count": 17
           },
           {
            "bin_end": 0.2175713436,
            "bin_start": 0.20950324139999998,
            "count": 44
           },
           {
            "bin_end": 0.2256394458,
            "bin_start": 0.2175713436,
            "count": 59
           },
           {
            "bin_end": 0.233707548,
            "bin_start": 0.2256394458,
            "count": 133
           },
           {
            "bin_end": 0.2417756502,
            "bin_start": 0.233707548,
            "count": 136
           },
           {
            "bin_end": 0.2498437524,
            "bin_start": 0.2417756502,
            "count": 120
           },
           {
            "bin_end": 0.2579118546,
            "bin_start": 0.2498437524,
            "count": 134
           },
           {
            "bin_end": 0.2659799568,
            "bin_start": 0.2579118546,
            "count": 164
           },
           {
            "bin_end": 0.274048059,
            "bin_start": 0.2659799568,
            "count": 95
           }
          ],
          "max": "0.274048059",
          "min": "0.193367037",
          "nan_count": 0,
          "unique_count": 848
         }
        },
        {
         "dtype": "float64",
         "name": "IQR",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.0354306005,
            "bin_start": 0.020098328,
            "count": 88
           },
           {
            "bin_end": 0.050762873,
            "bin_start": 0.0354306005,
            "count": 282
           },
           {
            "bin_end": 0.0660951455,
            "bin_start": 0.050762873,
            "count": 167
           },
           {
            "bin_end": 0.081427418,
            "bin_start": 0.0660951455,
            "count": 80
           },
           {
            "bin_end": 0.0967596905,
            "bin_start": 0.081427418,
            "count": 81
           },
           {
            "bin_end": 0.11209196299999999,
            "bin_start": 0.0967596905,
            "count": 113
           },
           {
            "bin_end": 0.12742423549999998,
            "bin_start": 0.11209196299999999,
            "count": 87
           },
           {
            "bin_end": 0.142756508,
            "bin_start": 0.12742423549999998,
            "count": 6
           },
           {
            "bin_end": 0.1580887805,
            "bin_start": 0.142756508,
            "count": 1
           },
           {
            "bin_end": 0.173421053,
            "bin_start": 0.1580887805,
            "count": 4
           }
          ],
          "max": "0.173421053",
          "min": "0.020098328",
          "nan_count": 0,
          "unique_count": 852
         }
        },
        {
         "dtype": "float64",
         "name": "skew",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.9930724056,
            "bin_start": 0.6811564840000001,
            "count": 18
           },
           {
            "bin_end": 1.3049883272,
            "bin_start": 0.9930724056,
            "count": 58
           },
           {
            "bin_end": 1.6169042488,
            "bin_start": 1.3049883272,
            "count": 146
           },
           {
            "bin_end": 1.9288201703999999,
            "bin_start": 1.6169042488,
            "count": 162
           },
           {
            "bin_end": 2.240736092,
            "bin_start": 1.9288201703999999,
            "count": 173
           },
           {
            "bin_end": 2.5526520136,
            "bin_start": 2.240736092,
            "count": 141
           },
           {
            "bin_end": 2.8645679352,
            "bin_start": 2.5526520136,
            "count": 95
           },
           {
            "bin_end": 3.1764838568,
            "bin_start": 2.8645679352,
            "count": 70
           },
           {
            "bin_end": 3.4883997784,
            "bin_start": 3.1764838568,
            "count": 33
           },
           {
            "bin_end": 3.8003157,
            "bin_start": 3.4883997784,
            "count": 13
           }
          ],
          "max": "3.8003157",
          "min": "0.6811564840000001",
          "nan_count": 0,
          "unique_count": 909
         }
        },
        {
         "dtype": "float64",
         "name": "kurt",
         "stats": {
          "histogram": [
           {
            "bin_end": 4.2139392432,
            "bin_start": 2.657216478,
            "count": 69
           },
           {
            "bin_end": 5.7706620083999995,
            "bin_start": 4.2139392432,
            "count": 181
           },
           {
            "bin_end": 7.327384773599999,
            "bin_start": 5.7706620083999995,
            "count": 177
           },
           {
            "bin_end": 8.884107538799999,
            "bin_start": 7.327384773599999,
            "count": 154
           },
           {
            "bin_end": 10.440830303999999,
            "bin_start": 8.884107538799999,
            "count": 95
           },
           {
            "bin_end": 11.997553069199999,
            "bin_start": 10.440830303999999,
            "count": 94
           },
           {
            "bin_end": 13.554275834399999,
            "bin_start": 11.997553069199999,
            "count": 48
           },
           {
            "bin_end": 15.110998599599998,
            "bin_start": 13.554275834399999,
            "count": 40
           },
           {
            "bin_end": 16.6677213648,
            "bin_start": 15.110998599599998,
            "count": 29
           },
           {
            "bin_end": 18.22444413,
            "bin_start": 16.6677213648,
            "count": 22
           }
          ],
          "max": "18.22444413",
          "min": "2.657216478",
          "nan_count": 0,
          "unique_count": 909
         }
        },
        {
         "dtype": "float64",
         "name": "sp.ent",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.8008363362,
            "bin_start": 0.783242909,
            "count": 8
           },
           {
            "bin_end": 0.8184297634000001,
            "bin_start": 0.8008363362,
            "count": 17
           },
           {
            "bin_end": 0.8360231906000001,
            "bin_start": 0.8184297634000001,
            "count": 67
           },
           {
            "bin_end": 0.8536166178000001,
            "bin_start": 0.8360231906000001,
            "count": 119
           },
           {
            "bin_end": 0.871210045,
            "bin_start": 0.8536166178000001,
            "count": 148
           },
           {
            "bin_end": 0.8888034722,
            "bin_start": 0.871210045,
            "count": 116
           },
           {
            "bin_end": 0.9063968994,
            "bin_start": 0.8888034722,
            "count": 125
           },
           {
            "bin_end": 0.9239903266,
            "bin_start": 0.9063968994,
            "count": 154
           },
           {
            "bin_end": 0.9415837538,
            "bin_start": 0.9239903266,
            "count": 126
           },
           {
            "bin_end": 0.959177181,
            "bin_start": 0.9415837538,
            "count": 29
           }
          ],
          "max": "0.959177181",
          "min": "0.783242909",
          "nan_count": 0,
          "unique_count": 909
         }
        },
        {
         "dtype": "float64",
         "name": "sfm",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.1377407484,
            "bin_start": 0.074133617,
            "count": 83
           },
           {
            "bin_end": 0.20134787979999996,
            "bin_start": 0.1377407484,
            "count": 201
           },
           {
            "bin_end": 0.2649550112,
            "bin_start": 0.20134787979999996,
            "count": 142
           },
           {
            "bin_end": 0.32856214259999994,
            "bin_start": 0.2649550112,
            "count": 115
           },
           {
            "bin_end": 0.3921692739999999,
            "bin_start": 0.32856214259999994,
            "count": 112
           },
           {
            "bin_end": 0.4557764053999999,
            "bin_start": 0.3921692739999999,
            "count": 122
           },
           {
            "bin_end": 0.5193835367999999,
            "bin_start": 0.4557764053999999,
            "count": 82
           },
           {
            "bin_end": 0.5829906681999999,
            "bin_start": 0.5193835367999999,
            "count": 35
           },
           {
            "bin_end": 0.6465977995999999,
            "bin_start": 0.5829906681999999,
            "count": 11
           },
           {
            "bin_end": 0.7102049309999999,
            "bin_start": 0.6465977995999999,
            "count": 6
           }
          ],
          "max": "0.7102049309999999",
          "min": "0.074133617",
          "nan_count": 0,
          "unique_count": 909
         }
        },
        {
         "dtype": "float64",
         "name": "mode",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.10407079629999991,
            "bin_start": 0.0845231069999999,
            "count": 31
           },
           {
            "bin_end": 0.12361848559999992,
            "bin_start": 0.10407079629999991,
            "count": 13
           },
           {
            "bin_end": 0.14316617489999994,
            "bin_start": 0.12361848559999992,
            "count": 19
           },
           {
            "bin_end": 0.16271386419999995,
            "bin_start": 0.14316617489999994,
            "count": 54
           },
           {
            "bin_end": 0.18226155349999995,
            "bin_start": 0.16271386419999995,
            "count": 81
           },
           {
            "bin_end": 0.20180924279999995,
            "bin_start": 0.18226155349999995,
            "count": 53
           },
           {
            "bin_end": 0.2213569321,
            "bin_start": 0.20180924279999995,
            "count": 117
           },
           {
            "bin_end": 0.2409046214,
            "bin_start": 0.2213569321,
            "count": 249
           },
           {
            "bin_end": 0.26045231069999997,
            "bin_start": 0.2409046214,
            "count": 26
           },
           {
            "bin_end": 0.28,
            "bin_start": 0.26045231069999997,
            "count": 266
           }
          ],
          "max": "0.28",
          "min": "0.0845231069999999",
          "nan_count": 0,
          "unique_count": 843
         }
        },
        {
         "dtype": "float64",
         "name": "centroid",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.1500290333,
            "bin_start": 0.13889185,
            "count": 4
           },
           {
            "bin_end": 0.16116621660000002,
            "bin_start": 0.1500290333,
            "count": 11
           },
           {
            "bin_end": 0.17230339990000001,
            "bin_start": 0.16116621660000002,
            "count": 34
           },
           {
            "bin_end": 0.1834405832,
            "bin_start": 0.17230339990000001,
            "count": 137
           },
           {
            "bin_end": 0.19457776650000003,
            "bin_start": 0.1834405832,
            "count": 141
           },
           {
            "bin_end": 0.20571494980000002,
            "bin_start": 0.19457776650000003,
            "count": 113
           },
           {
            "bin_end": 0.2168521331,
            "bin_start": 0.20571494980000002,
            "count": 131
           },
           {
            "bin_end": 0.22798931640000003,
            "bin_start": 0.2168521331,
            "count": 136
           },
           {
            "bin_end": 0.23912649970000002,
            "bin_start": 0.22798931640000003,
            "count": 148
           },
           {
            "bin_end": 0.250263683,
            "bin_start": 0.23912649970000002,
            "count": 54
           }
          ],
          "max": "0.250263683",
          "min": "0.13889185",
          "nan_count": 0,
          "unique_count": 909
         }
        },
        {
         "dtype": "float64",
         "name": "meanfun",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.0900545828999999,
            "bin_start": 0.0746602359999999,
            "count": 6
           },
           {
            "bin_end": 0.10544892979999992,
            "bin_start": 0.0900545828999999,
            "count": 41
           },
           {
            "bin_end": 0.12084327669999993,
            "bin_start": 0.10544892979999992,
            "count": 63
           },
           {
            "bin_end": 0.13623762359999994,
            "bin_start": 0.12084327669999993,
            "count": 95
           },
           {
            "bin_end": 0.15163197049999994,
            "bin_start": 0.13623762359999994,
            "count": 82
           },
           {
            "bin_end": 0.16702631739999996,
            "bin_start": 0.15163197049999994,
            "count": 101
           },
           {
            "bin_end": 0.18242066429999998,
            "bin_start": 0.16702631739999996,
            "count": 173
           },
           {
            "bin_end": 0.1978150112,
            "bin_start": 0.18242066429999998,
            "count": 193
           },
           {
            "bin_end": 0.21320935810000002,
            "bin_start": 0.1978150112,
            "count": 119
           },
           {
            "bin_end": 0.228603705,
            "bin_start": 0.21320935810000002,
            "count": 36
           }
          ],
          "max": "0.228603705",
          "min": "0.0746602359999999",
          "nan_count": 0,
          "unique_count": 909
         }
        },
        {
         "dtype": "float64",
         "name": "minfun",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.0227718988,
            "bin_start": 0.015640274,
            "count": 407
           },
           {
            "bin_end": 0.0299035236,
            "bin_start": 0.0227718988,
            "count": 134
           },
           {
            "bin_end": 0.037035148399999995,
            "bin_start": 0.0299035236,
            "count": 104
           },
           {
            "bin_end": 0.044166773199999995,
            "bin_start": 0.037035148399999995,
            "count": 73
           },
           {
            "bin_end": 0.051298397999999995,
            "bin_start": 0.044166773199999995,
            "count": 45
           },
           {
            "bin_end": 0.058430022799999995,
            "bin_start": 0.051298397999999995,
            "count": 42
           },
           {
            "bin_end": 0.0655616476,
            "bin_start": 0.058430022799999995,
            "count": 25
           },
           {
            "bin_end": 0.0726932724,
            "bin_start": 0.0655616476,
            "count": 32
           },
           {
            "bin_end": 0.0798248972,
            "bin_start": 0.0726932724,
            "count": 25
           },
           {
            "bin_end": 0.086956522,
            "bin_start": 0.0798248972,
            "count": 22
           }
          ],
          "max": "0.086956522",
          "min": "0.015640274",
          "nan_count": 0,
          "unique_count": 549
         }
        },
        {
         "dtype": "float64",
         "name": "maxfun",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.2010801824999999,
            "bin_start": 0.1927710839999999,
            "count": 11
           },
           {
            "bin_end": 0.20938928099999993,
            "bin_start": 0.2010801824999999,
            "count": 5
           },
           {
            "bin_end": 0.21769837949999993,
            "bin_start": 0.20938928099999993,
            "count": 17
           },
           {
            "bin_end": 0.22600747799999993,
            "bin_start": 0.21769837949999993,
            "count": 39
           },
           {
            "bin_end": 0.23431657649999993,
            "bin_start": 0.22600747799999993,
            "count": 28
           },
           {
            "bin_end": 0.24262567499999996,
            "bin_start": 0.23431657649999993,
            "count": 93
           },
           {
            "bin_end": 0.2509347735,
            "bin_start": 0.24262567499999996,
            "count": 71
           },
           {
            "bin_end": 0.259243872,
            "bin_start": 0.2509347735,
            "count": 92
           },
           {
            "bin_end": 0.2675529705,
            "bin_start": 0.259243872,
            "count": 150
           },
           {
            "bin_end": 0.275862069,
            "bin_start": 0.2675529705,
            "count": 403
           }
          ],
          "max": "0.275862069",
          "min": "0.1927710839999999",
          "nan_count": 0,
          "unique_count": 25
         }
        },
        {
         "dtype": "float64",
         "name": "meandom",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.2690760104,
            "bin_start": 0.107319079,
            "count": 24
           },
           {
            "bin_end": 0.4308329418,
            "bin_start": 0.2690760104,
            "count": 89
           },
           {
            "bin_end": 0.5925898732,
            "bin_start": 0.4308329418,
            "count": 165
           },
           {
            "bin_end": 0.7543468046,
            "bin_start": 0.5925898732,
            "count": 164
           },
           {
            "bin_end": 0.9161037360000001,
            "bin_start": 0.7543468046,
            "count": 149
           },
           {
            "bin_end": 1.0778606674,
            "bin_start": 0.9161037360000001,
            "count": 148
           },
           {
            "bin_end": 1.2396175988,
            "bin_start": 1.0778606674,
            "count": 82
           },
           {
            "bin_end": 1.4013745302,
            "bin_start": 1.2396175988,
            "count": 52
           },
           {
            "bin_end": 1.5631314616,
            "bin_start": 1.4013745302,
            "count": 25
           },
           {
            "bin_end": 1.724888393,
            "bin_start": 1.5631314616,
            "count": 11
           }
          ],
          "max": "1.724888393",
          "min": "0.107319079",
          "nan_count": 0,
          "unique_count": 867
         }
        },
        {
         "dtype": "float64",
         "name": "mindom",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.05,
            "bin_start": 0,
            "count": 265
           },
           {
            "bin_end": 0.1,
            "bin_start": 0.05,
            "count": 99
           },
           {
            "bin_end": 0.15000000000000002,
            "bin_start": 0.1,
            "count": 101
           },
           {
            "bin_end": 0.2,
            "bin_start": 0.15000000000000002,
            "count": 203
           },
           {
            "bin_end": 0.25,
            "bin_start": 0.2,
            "count": 155
           },
           {
            "bin_end": 0.30000000000000004,
            "bin_start": 0.25,
            "count": 50
           },
           {
            "bin_end": 0.35000000000000003,
            "bin_start": 0.30000000000000004,
            "count": 20
           },
           {
            "bin_end": 0.4,
            "bin_start": 0.35000000000000003,
            "count": 6
           },
           {
            "bin_end": 0.45,
            "bin_start": 0.4,
            "count": 5
           },
           {
            "bin_end": 0.5,
            "bin_start": 0.45,
            "count": 5
           }
          ],
          "max": "0.5",
          "min": "0.0",
          "nan_count": 0,
          "unique_count": 56
         }
        },
        {
         "dtype": "float64",
         "name": "maxdom",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.91015625,
            "bin_start": 0.234375,
            "count": 97
           },
           {
            "bin_end": 1.5859375,
            "bin_start": 0.91015625,
            "count": 46
           },
           {
            "bin_end": 2.26171875,
            "bin_start": 1.5859375,
            "count": 19
           },
           {
            "bin_end": 2.9375,
            "bin_start": 2.26171875,
            "count": 64
           },
           {
            "bin_end": 3.61328125,
            "bin_start": 2.9375,
            "count": 107
           },
           {
            "bin_end": 4.2890625,
            "bin_start": 3.61328125,
            "count": 91
           },
           {
            "bin_end": 4.96484375,
            "bin_start": 4.2890625,
            "count": 100
           },
           {
            "bin_end": 5.640625,
            "bin_start": 4.96484375,
            "count": 103
           },
           {
            "bin_end": 6.31640625,
            "bin_start": 5.640625,
            "count": 134
           },
           {
            "bin_end": 6.9921875,
            "bin_start": 6.31640625,
            "count": 148
           }
          ],
          "max": "6.9921875",
          "min": "0.234375",
          "nan_count": 0,
          "unique_count": 520
         }
        },
        {
         "dtype": "float64",
         "name": "dfrange",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.7765625,
            "bin_start": 0.0859375,
            "count": 95
           },
           {
            "bin_end": 1.4671875,
            "bin_start": 0.7765625,
            "count": 49
           },
           {
            "bin_end": 2.1578125000000004,
            "bin_start": 1.4671875,
            "count": 22
           },
           {
            "bin_end": 2.8484375,
            "bin_start": 2.1578125000000004,
            "count": 65
           },
           {
            "bin_end": 3.5390625,
            "bin_start": 2.8484375,
            "count": 111
           },
           {
            "bin_end": 4.229687500000001,
            "bin_start": 3.5390625,
            "count": 90
           },
           {
            "bin_end": 4.9203125000000005,
            "bin_start": 4.229687500000001,
            "count": 97
           },
           {
            "bin_end": 5.6109375,
            "bin_start": 4.9203125000000005,
            "count": 119
           },
           {
            "bin_end": 6.3015625,
            "bin_start": 5.6109375,
            "count": 133
           },
           {
            "bin_end": 6.9921875,
            "bin_start": 6.3015625,
            "count": 128
           }
          ],
          "max": "6.9921875",
          "min": "0.0859375",
          "nan_count": 0,
          "unique_count": 524
         }
        },
        {
         "dtype": "float64",
         "name": "modindx",
         "stats": {
          "histogram": [
           {
            "bin_end": 0.0490012997,
            "bin_start": 0.009846044,
            "count": 3
           },
           {
            "bin_end": 0.0881565554,
            "bin_start": 0.0490012997,
            "count": 11
           },
           {
            "bin_end": 0.1273118111,
            "bin_start": 0.0881565554,
            "count": 98
           },
           {
            "bin_end": 0.1664670668,
            "bin_start": 0.1273118111,
            "count": 176
           },
           {
            "bin_end": 0.2056223225,
            "bin_start": 0.1664670668,
            "count": 232
           },
           {
            "bin_end": 0.2447775782,
            "bin_start": 0.2056223225,
            "count": 158
           },
           {
            "bin_end": 0.2839328339,
            "bin_start": 0.2447775782,
            "count": 100
           },
           {
            "bin_end": 0.3230880896,
            "bin_start": 0.2839328339,
            "count": 55
           },
           {
            "bin_end": 0.3622433453,
            "bin_start": 0.3230880896,
            "count": 52
           },
           {
            "bin_end": 0.401398601,
            "bin_start": 0.3622433453,
            "count": 24
           }
          ],
          "max": "0.401398601",
          "min": "0.009846044",
          "nan_count": 0,
          "unique_count": 906
         }
        },
        {
         "dtype": "object",
         "name": "label",
         "stats": {
          "categories": [
           {
            "count": 314,
            "name": "sad"
           },
           {
            "count": 302,
            "name": "angry"
           },
           {
            "count": 293,
            "name": "happy"
           }
          ],
          "nan_count": 0,
          "unique_count": 3
         }
        },
        {
         "dtype": "int64",
         "name": "_deepnote_index_column"
        }
       ],
       "row_count": 909,
       "rows": [
        {
         "IQR": "0.107388949",
         "Q25": "0.126197183",
         "Q75": "0.233586132",
         "_deepnote_index_column": "0",
         "centroid": "0.181338105",
         "dfrange": "6.140625",
         "kurt": "2.863716892",
         "label": "sad",
         "maxdom": "6.2265625",
         "maxfun": "0.271186441",
         "meandom": "0.77734375",
         "meanfreq": "0.181338105",
         "meanfun": "0.137742223",
         "median": "0.187475623",
         "mindom": "0.0859375",
         "minfun": "0.0230215829999999",
         "mode": "0.216901408",
         "modindx": "0.116585704",
         "sd": "0.0604951159999999",
         "sfm": "0.307219558",
         "skew": "0.8690878000000001",
         "sp.ent": "0.923566258"
        },
        {
         "IQR": "0.113140407",
         "Q25": "0.130846731",
         "Q75": "0.243987138",
         "_deepnote_index_column": "1",
         "centroid": "0.186897392",
         "dfrange": "3.9140625",
         "kurt": "3.878650362",
         "label": "sad",
         "maxdom": "4.0",
         "maxfun": "0.271186441",
         "meandom": "0.930338542",
         "meanfreq": "0.186897392",
         "meanfun": "0.121811357",
         "median": "0.195069668",
         "mindom": "0.0859375",
         "minfun": "0.018411968",
         "mode": "0.135648446",
         "modindx": "0.144982762",
         "sd": "0.062260334",
         "sfm": "0.298859325",
         "skew": "1.191767427",
         "sp.ent": "0.91884764"
        },
        {
         "IQR": "0.118555799",
         "Q25": "0.131422319",
         "Q75": "0.249978118",
         "_deepnote_index_column": "2",
         "centroid": "0.189102142",
         "dfrange": "0.5390625",
         "kurt": "4.5899950480000005",
         "label": "sad",
         "maxdom": "0.625",
         "maxfun": "0.262295082",
         "meandom": "0.332386364",
         "meanfreq": "0.189102142",
         "meanfun": "0.123757895",
         "median": "0.204945295",
         "mindom": "0.0859375",
         "minfun": "0.083333333",
         "mode": "0.264070022",
         "modindx": "0.334782609",
         "sd": "0.0629012759999999",
         "sfm": "0.313068592",
         "skew": "1.312690276",
         "sp.ent": "0.91951906"
        },
        {
         "IQR": "0.107017189",
         "Q25": "0.129949444",
         "Q75": "0.236966633",
         "_deepnote_index_column": "3",
         "centroid": "0.183036061",
         "dfrange": "5.3828125",
         "kurt": "3.680995381",
         "label": "sad",
         "maxdom": "5.46875",
         "maxfun": "0.258064516",
         "meandom": "1.012019231",
         "meanfreq": "0.183036061",
         "meanfun": "0.128469334",
         "median": "0.174115268",
         "mindom": "0.0859375",
         "minfun": "0.044692737",
         "mode": "0.152032356",
         "modindx": "0.304910498",
         "sd": "0.060051263",
         "sfm": "0.329295377",
         "skew": "1.096408695",
         "sp.ent": "0.921361417"
        },
        {
         "IQR": "0.099543198",
         "Q25": "0.1167825219999999",
         "Q75": "0.21632572",
         "_deepnote_index_column": "4",
         "centroid": "0.16879306",
         "dfrange": "0.65625",
         "kurt": "5.031743902",
         "label": "sad",
         "maxdom": "0.75",
         "maxfun": "0.235294118",
         "meandom": "0.228794643",
         "meanfreq": "0.16879306",
         "meanfun": "0.109719895",
         "median": "0.156266137",
         "mindom": "0.09375",
         "minfun": "0.02247191",
         "mode": "0.153763654",
         "modindx": "0.306776557",
         "sd": "0.057910224",
         "sfm": "0.337046772",
         "skew": "1.3868367469999998",
         "sp.ent": "0.926237688"
        },
        {
         "IQR": "0.114309278",
         "Q25": "0.114309278",
         "Q75": "0.228618557",
         "_deepnote_index_column": "5",
         "centroid": "0.174928732",
         "dfrange": "0.53125",
         "kurt": "8.529236346",
         "label": "sad",
         "maxdom": "0.625",
         "maxfun": "0.23880597",
         "meandom": "0.2734375",
         "meanfreq": "0.174928732",
         "meanfun": "0.116962212",
         "median": "0.19628866",
         "mindom": "0.09375",
         "minfun": "0.030534351",
         "mode": "0.114020619",
         "modindx": "0.270833333",
         "sd": "0.059598251",
         "sfm": "0.292016541",
         "skew": "2.212273995",
         "sp.ent": "0.8973973540000001"
        },
        {
         "IQR": "0.118918919",
         "Q25": "0.104594595",
         "Q75": "0.223513514",
         "_deepnote_index_column": "6",
         "centroid": "0.165810718",
         "dfrange": "0.421875",
         "kurt": "7.819853636",
         "label": "sad",
         "maxdom": "0.5078125",
         "maxfun": "0.262295082",
         "meandom": "0.172916667",
         "meanfreq": "0.165810718",
         "meanfun": "0.092203064",
         "median": "0.170540541",
         "mindom": "0.0859375",
         "minfun": "0.017204301",
         "mode": "0.091081081",
         "modindx": "0.335978836",
         "sd": "0.063460422",
         "sfm": "0.322477646",
         "skew": "2.026813112",
         "sp.ent": "0.906607039"
        },
        {
         "IQR": "0.1185",
         "Q25": "0.11475",
         "Q75": "0.23325",
         "_deepnote_index_column": "7",
         "centroid": "0.174916076",
         "dfrange": "4.796875",
         "kurt": "5.073168153",
         "label": "sad",
         "maxdom": "4.890625",
         "maxfun": "0.235294118",
         "meandom": "0.544270833",
         "meanfreq": "0.174916076",
         "meanfun": "0.096092374",
         "median": "0.1675",
         "mindom": "0.09375",
         "minfun": "0.0182857139999999",
         "mode": "0.111",
         "modindx": "0.151849013",
         "sd": "0.06426611",
         "sfm": "0.335891646",
         "skew": "1.425661954",
         "sp.ent": "0.918925452"
        },
        {
         "IQR": "0.117857143",
         "Q25": "0.116428571",
         "Q75": "0.2342857139999999",
         "_deepnote_index_column": "8",
         "centroid": "0.175010065",
         "dfrange": "3.7109375",
         "kurt": "10.71239016",
         "label": "sad",
         "maxdom": "3.796875",
         "maxfun": "0.235294118",
         "meandom": "0.727384868",
         "meanfreq": "0.175010065",
         "meanfun": "0.1096915109999999",
         "median": "0.162857143",
         "mindom": "0.0859375",
         "minfun": "0.015888779",
         "mode": "0.114761905",
         "modindx": "0.277894737",
         "sd": "0.062859957",
         "sfm": "0.279586048",
         "skew": "2.50498167",
         "sp.ent": "0.8985707490000001"
        },
        {
         "IQR": "0.1169999999999999",
         "Q25": "0.114",
         "Q75": "0.231",
         "_deepnote_index_column": "9",
         "centroid": "0.173529015",
         "dfrange": "5.078125",
         "kurt": "11.54236629",
         "label": "sad",
         "maxdom": "5.15625",
         "maxfun": "0.275862069",
         "meandom": "0.917100694",
         "meanfreq": "0.173529015",
         "meanfun": "0.110870419",
         "median": "0.18275",
         "mindom": "0.078125",
         "minfun": "0.017204301",
         "mode": "0.1155",
         "modindx": "0.258653846",
         "sd": "0.06273042",
         "sfm": "0.30838928",
         "skew": "2.468755708",
         "sp.ent": "0.905506498"
        }
       ]
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.187476</td>\n",
       "      <td>0.126197</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.869088</td>\n",
       "      <td>2.863717</td>\n",
       "      <td>0.923566</td>\n",
       "      <td>0.307220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.137742</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>6.226562</td>\n",
       "      <td>6.140625</td>\n",
       "      <td>0.116586</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.195070</td>\n",
       "      <td>0.130847</td>\n",
       "      <td>0.243987</td>\n",
       "      <td>0.113140</td>\n",
       "      <td>1.191767</td>\n",
       "      <td>3.878650</td>\n",
       "      <td>0.918848</td>\n",
       "      <td>0.298859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.121811</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.930339</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.914062</td>\n",
       "      <td>0.144983</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.062901</td>\n",
       "      <td>0.204945</td>\n",
       "      <td>0.131422</td>\n",
       "      <td>0.249978</td>\n",
       "      <td>0.118556</td>\n",
       "      <td>1.312690</td>\n",
       "      <td>4.589995</td>\n",
       "      <td>0.919519</td>\n",
       "      <td>0.313069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.123758</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.332386</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.174115</td>\n",
       "      <td>0.129949</td>\n",
       "      <td>0.236967</td>\n",
       "      <td>0.107017</td>\n",
       "      <td>1.096409</td>\n",
       "      <td>3.680995</td>\n",
       "      <td>0.921361</td>\n",
       "      <td>0.329295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.128469</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>1.012019</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>5.468750</td>\n",
       "      <td>5.382812</td>\n",
       "      <td>0.304910</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.156266</td>\n",
       "      <td>0.116783</td>\n",
       "      <td>0.216326</td>\n",
       "      <td>0.099543</td>\n",
       "      <td>1.386837</td>\n",
       "      <td>5.031744</td>\n",
       "      <td>0.926238</td>\n",
       "      <td>0.337047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.109720</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.228795</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.306777</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>0.244013</td>\n",
       "      <td>0.035477</td>\n",
       "      <td>0.254385</td>\n",
       "      <td>0.229653</td>\n",
       "      <td>0.265573</td>\n",
       "      <td>0.035920</td>\n",
       "      <td>2.214752</td>\n",
       "      <td>7.565052</td>\n",
       "      <td>0.821874</td>\n",
       "      <td>0.136933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244013</td>\n",
       "      <td>0.202433</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.616536</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>1.609375</td>\n",
       "      <td>1.398438</td>\n",
       "      <td>0.281869</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>0.235383</td>\n",
       "      <td>0.045303</td>\n",
       "      <td>0.248974</td>\n",
       "      <td>0.220745</td>\n",
       "      <td>0.264233</td>\n",
       "      <td>0.043488</td>\n",
       "      <td>2.474743</td>\n",
       "      <td>9.959019</td>\n",
       "      <td>0.848109</td>\n",
       "      <td>0.236957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235383</td>\n",
       "      <td>0.189293</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.115723</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>5.234375</td>\n",
       "      <td>0.167861</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>0.231211</td>\n",
       "      <td>0.044793</td>\n",
       "      <td>0.234847</td>\n",
       "      <td>0.221477</td>\n",
       "      <td>0.262090</td>\n",
       "      <td>0.040613</td>\n",
       "      <td>2.607668</td>\n",
       "      <td>10.698821</td>\n",
       "      <td>0.848702</td>\n",
       "      <td>0.241998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231211</td>\n",
       "      <td>0.171805</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.070801</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>4.554688</td>\n",
       "      <td>4.289062</td>\n",
       "      <td>0.214936</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>0.213587</td>\n",
       "      <td>0.082267</td>\n",
       "      <td>0.249435</td>\n",
       "      <td>0.207680</td>\n",
       "      <td>0.268538</td>\n",
       "      <td>0.060858</td>\n",
       "      <td>3.460579</td>\n",
       "      <td>18.034614</td>\n",
       "      <td>0.882544</td>\n",
       "      <td>0.425394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213587</td>\n",
       "      <td>0.155277</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1.724888</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>6.812500</td>\n",
       "      <td>6.539062</td>\n",
       "      <td>0.238857</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.078746</td>\n",
       "      <td>0.245034</td>\n",
       "      <td>0.209794</td>\n",
       "      <td>0.264031</td>\n",
       "      <td>0.054238</td>\n",
       "      <td>2.563983</td>\n",
       "      <td>10.392885</td>\n",
       "      <td>0.887389</td>\n",
       "      <td>0.404993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.162938</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.915625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.281250</td>\n",
       "      <td>6.281250</td>\n",
       "      <td>0.193141</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>909 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     meanfreq        sd    median       Q25       Q75       IQR      skew  \\\n",
       "0    0.181338  0.060495  0.187476  0.126197  0.233586  0.107389  0.869088   \n",
       "1    0.186897  0.062260  0.195070  0.130847  0.243987  0.113140  1.191767   \n",
       "2    0.189102  0.062901  0.204945  0.131422  0.249978  0.118556  1.312690   \n",
       "3    0.183036  0.060051  0.174115  0.129949  0.236967  0.107017  1.096409   \n",
       "4    0.168793  0.057910  0.156266  0.116783  0.216326  0.099543  1.386837   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "904  0.244013  0.035477  0.254385  0.229653  0.265573  0.035920  2.214752   \n",
       "905  0.235383  0.045303  0.248974  0.220745  0.264233  0.043488  2.474743   \n",
       "906  0.231211  0.044793  0.234847  0.221477  0.262090  0.040613  2.607668   \n",
       "907  0.213587  0.082267  0.249435  0.207680  0.268538  0.060858  3.460579   \n",
       "908  0.212537  0.078746  0.245034  0.209794  0.264031  0.054238  2.563983   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n",
       "0     2.863717  0.923566  0.307220  ...  0.181338  0.137742  0.023022   \n",
       "1     3.878650  0.918848  0.298859  ...  0.186897  0.121811  0.018412   \n",
       "2     4.589995  0.919519  0.313069  ...  0.189102  0.123758  0.083333   \n",
       "3     3.680995  0.921361  0.329295  ...  0.183036  0.128469  0.044693   \n",
       "4     5.031744  0.926238  0.337047  ...  0.168793  0.109720  0.022472   \n",
       "..         ...       ...       ...  ...       ...       ...       ...   \n",
       "904   7.565052  0.821874  0.136933  ...  0.244013  0.202433  0.028829   \n",
       "905   9.959019  0.848109  0.236957  ...  0.235383  0.189293  0.031250   \n",
       "906  10.698821  0.848702  0.241998  ...  0.231211  0.171805  0.022346   \n",
       "907  18.034614  0.882544  0.425394  ...  0.213587  0.155277  0.020592   \n",
       "908  10.392885  0.887389  0.404993  ...  0.212537  0.162938  0.024845   \n",
       "\n",
       "       maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0    0.271186  0.777344  0.085938  6.226562  6.140625  0.116586    sad  \n",
       "1    0.271186  0.930339  0.085938  4.000000  3.914062  0.144983    sad  \n",
       "2    0.262295  0.332386  0.085938  0.625000  0.539062  0.334783    sad  \n",
       "3    0.258065  1.012019  0.085938  5.468750  5.382812  0.304910    sad  \n",
       "4    0.235294  0.228795  0.093750  0.750000  0.656250  0.306777    sad  \n",
       "..        ...       ...       ...       ...       ...       ...    ...  \n",
       "904  0.271186  0.616536  0.210938  1.609375  1.398438  0.281869  happy  \n",
       "905  0.275862  1.115723  0.265625  5.500000  5.234375  0.167861  happy  \n",
       "906  0.275862  1.070801  0.265625  4.554688  4.289062  0.214936  happy  \n",
       "907  0.275862  1.724888  0.273438  6.812500  6.539062  0.238857  happy  \n",
       "908  0.258065  0.915625  0.000000  6.281250  6.281250  0.193141  happy  \n",
       "\n",
       "[909 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(labels = ['Unnamed: 0', 'Unnamed: 0.1', 'X'], axis = 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "070e68d033094a4aa0e804ec8d7f0651",
    "deepnote_app_coordinates": {
     "h": 6,
     "w": 12,
     "x": 0,
     "y": 132
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Nos hacemos unas _**funciones para estandarizar**_ y probar el rendimiento del modelo con las distintas estandarizaciones, y sin alguna estandarizaciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "3f01408ec282481d8a460da536faf0d5",
    "deepnote_app_coordinates": {
     "h": 24,
     "w": 12,
     "x": 0,
     "y": 139
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1681601818098,
    "source_hash": "48e28569"
   },
   "outputs": [],
   "source": [
    "def data_scaling(data: pd.DataFrame,\n",
    "                 scaler: sklearn.preprocessing._data,\n",
    "                 scaling_columns     = [], \n",
    "                 not_scaling_columns = []):\n",
    "    \n",
    "    \"\"\"\n",
    "    Scales the specified columns in the input DataFrame using the given scaler object.\n",
    "    \n",
    "    INPUT:\n",
    "        - data:                Pandas DataFrame to be scaled\n",
    "        - scaler:              Scaler object from sklearn.preprocessing\n",
    "        - scaling_columns:     List of column names to scale\n",
    "        - not_scaling_columns: List of column names to not scale\n",
    "        \n",
    "    OUTPUT: \n",
    "        - pd.DataFrame:        A scaled copy of the input data with the specified columns scaled\n",
    "    \"\"\"\n",
    "\n",
    "    # if the user provided all the columns he wishes to scale \n",
    "    if scaling_columns:\n",
    "        \n",
    "        data[scaling_columns].iloc[:] = scaler.fit_transform(data[scaling_columns].iloc[:].to_numpy())\n",
    "\n",
    "    # if user provided only the columns he doesn't want to scale\n",
    "    elif not_scaling_columns:\n",
    "        \n",
    "        used_cols = data.columns.difference(not_scaling_columns)\n",
    "        data.loc[:, used_cols] = scaler.fit_transform(data.loc[:, used_cols].to_numpy())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "3ce91b92b2f94623950c9131c3b8f056",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 164
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 50,
    "execution_start": 1681601818147,
    "source_hash": "79350299"
   },
   "outputs": [],
   "source": [
    "X  = data_scaling(data = X, scaler = StandardScaler(), not_scaling_columns = ['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "60d4dbe161964c4a97f2cb71bbbf8cd9",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 169
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "TambiÃ©n _**creamos variables dummy**_ para la variable categÃ³rica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "6cef66c936b64a2cb3e87f442fb4ae62",
    "deepnote_app_coordinates": {
     "h": 27,
     "w": 12,
     "x": 0,
     "y": 174
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1681601818196,
    "source_hash": "112e4862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 909 entries, 0 to 908\n",
      "Data columns (total 23 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   meanfreq     909 non-null    float64\n",
      " 1   sd           909 non-null    float64\n",
      " 2   median       909 non-null    float64\n",
      " 3   Q25          909 non-null    float64\n",
      " 4   Q75          909 non-null    float64\n",
      " 5   IQR          909 non-null    float64\n",
      " 6   skew         909 non-null    float64\n",
      " 7   kurt         909 non-null    float64\n",
      " 8   sp.ent       909 non-null    float64\n",
      " 9   sfm          909 non-null    float64\n",
      " 10  mode         909 non-null    float64\n",
      " 11  centroid     909 non-null    float64\n",
      " 12  meanfun      909 non-null    float64\n",
      " 13  minfun       909 non-null    float64\n",
      " 14  maxfun       909 non-null    float64\n",
      " 15  meandom      909 non-null    float64\n",
      " 16  mindom       909 non-null    float64\n",
      " 17  maxdom       909 non-null    float64\n",
      " 18  dfrange      909 non-null    float64\n",
      " 19  modindx      909 non-null    float64\n",
      " 20  label_angry  909 non-null    uint8  \n",
      " 21  label_happy  909 non-null    uint8  \n",
      " 22  label_sad    909 non-null    uint8  \n",
      "dtypes: float64(20), uint8(3)\n",
      "memory usage: 144.8 KB\n"
     ]
    }
   ],
   "source": [
    "X = pd.get_dummies(X, columns = ['label'])\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a7d9df7326d9499996b207472c7a3b9e",
    "deepnote_app_coordinates": {
     "h": 20,
     "w": 12,
     "x": 0,
     "y": 202
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### CreaciÃ³n de los conjuntos de entrenamiento, validaciÃ³n y test\n",
    "\n",
    "Ahora, creamos los conjuntos de entrenamiento, validaciÃ³n y test. Vamos a usar una proporciÃ³n de:\n",
    "\n",
    "- Para test un 10%.\n",
    "- Para validaciÃ³n un 10%.\n",
    "- Y el resto para train.\n",
    "\n",
    "TambiÃ©n creamos una _**clase Dataset**_ que nos prepara los datos para alimentar la red neuronal, convirtiendolos a parejas ordenadas de tensores conteniento los atributos y la variable objetivo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "c4f06243d30447bbbe7e60dce0194f36",
    "deepnote_app_coordinates": {
     "h": 15,
     "w": 12,
     "x": 0,
     "y": 223
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1681601818247,
    "source_hash": "10f72150"
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    " \n",
    "  def __init__(self,\n",
    "               df: pd.DataFrame,\n",
    "               target_column: list):\n",
    "\n",
    "    y = df[target_column].values\n",
    "    X = df.drop(target_column,axis=1).values\n",
    "\n",
    "    self.X = torch.tensor(X, dtype=torch.float32)\n",
    "    self.y = torch.tensor(y, dtype=torch.float32)\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.y)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "b6fc1e2414864e14bb8f067f5f8dc423",
    "deepnote_app_coordinates": {
     "h": 25,
     "w": 12,
     "x": 0,
     "y": 239
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1681601818247,
    "source_hash": "26905485"
   },
   "outputs": [],
   "source": [
    "def train_test_val_split(data: pd.DataFrame, \n",
    "                         proportions = [0.8, 0.1, 0.1]):\n",
    "\n",
    "    \"\"\"\n",
    "        Splits the input DataFrame into train, test, and validation sets based on the given proportions.\n",
    "\n",
    "        INPUT:\n",
    "            - data:        The dataframe to be splitted\n",
    "\n",
    "            - proportions: List of proportions of size of each partition of the dataframe, first element \n",
    "                           of the list corresponds to the proportion of train, the 2nd one indicates the\n",
    "                           test proportion, and the last element is the validation proportion\n",
    "\n",
    "        OUTPUT: \n",
    "            - (pd.DataFrame, pd.DataFrame, pd.DataFrame): in the form\n",
    "              (train_dataset, test_dataset, validation_dataset)\n",
    "    \"\"\"\n",
    "\n",
    "    data_len = len(data)\n",
    "\n",
    "    #end of train\n",
    "    train_indx = int(proportions[0]*data_len)\n",
    "\n",
    "    #end of test\n",
    "    test_indx = train_indx + int(proportions[1]*data_len)\n",
    "\n",
    "    out = (data.iloc[:train_indx],\n",
    "           data.iloc[train_indx+1:test_indx],\n",
    "           data.iloc[test_indx+1:-1])\n",
    "           \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "4057a28ad8514c9dbd8a305bd41c2726",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 265
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 61,
    "execution_start": 1681601818247,
    "source_hash": "76f20659"
   },
   "outputs": [],
   "source": [
    "train, test, val = train_test_val_split(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "175fac5824f940b9baac432d39254692",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 270
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### PreparaciÃ³n de los datos para alimentar la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "64525e52d60e40e5af7621b758899476",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 276
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Continuamos preparando los datos, convirtiendolos a tensores, y creando los dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "9e812cbbb21440c2a9eae01dfb08860b",
    "deepnote_app_coordinates": {
     "h": 7,
     "w": 12,
     "x": 0,
     "y": 281
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 25,
    "execution_start": 1681601818285,
    "source_hash": "ffd58a99"
   },
   "outputs": [],
   "source": [
    "outcome_vars = ['label_angry', 'label_happy', 'label_sad']\n",
    "\n",
    "# Use the Dataset class to prepare each array in the form of tensors\n",
    "train_sec = Dataset(train, outcome_vars)\n",
    "test_sec  = Dataset(test, outcome_vars)\n",
    "val_sec   = Dataset(val, outcome_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f86a952f2ac84bdebc016f4dfe5a9690",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 289
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "AcontinuaciÃ³n puede ver el tamaÃ±o de nuestro set de entrenamiento, testeo y validaciÃ³n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "85f11a073b0b4db1bed8ac0bfa71415f",
    "deepnote_app_coordinates": {
     "h": 6,
     "w": 12,
     "x": 0,
     "y": 295
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1681601818306,
    "source_hash": "33ef3267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: torch.Size([727, 20]), test: torch.Size([89, 20]), val: torch.Size([90, 20])\n"
     ]
    }
   ],
   "source": [
    "s = f\"train: {train_sec.X.shape}, test: {test_sec.X.shape}, val: {val_sec.X.shape}\"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "2286ef002c0d4fbe83b0cecf22a34ffa",
    "deepnote_app_coordinates": {
     "h": 20,
     "w": 12,
     "x": 0,
     "y": 302
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1681601818314,
    "source_hash": "3bc8e5a4"
   },
   "outputs": [],
   "source": [
    "# Define the DataLoaders to load the information in batches\n",
    "train_data = DataLoader(\n",
    "    train_sec,\n",
    "    batch_size = 16,\n",
    "    shuffle    = False,\n",
    " )\n",
    "\n",
    "test_data = DataLoader(\n",
    "    test_sec,\n",
    "    batch_size  = 16,\n",
    "    shuffle     = False,\n",
    "    #num_workers = 0,\n",
    "    #collate_fn  = None,\n",
    "    #pin_memory  = False,\n",
    " )\n",
    "\n",
    "val_data = DataLoader(\n",
    "    val_sec,\n",
    "    batch_size  = 16,\n",
    "    shuffle     = False,\n",
    "    #num_workers = 0,\n",
    "    #collate_fn  = None,\n",
    "    #pin_memory  = False,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "27f561315fb1498980f18ee9e909abe2",
    "deepnote_app_coordinates": {
     "h": 8,
     "w": 12,
     "x": 0,
     "y": 323
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### DefiniciÃ³n de la clase Net, el optimizador, la funciÃ³n de costo y el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "26257ae4f58e4094924139c627090bf1",
    "deepnote_app_coordinates": {
     "h": 49,
     "w": 12,
     "x": 0,
     "y": 332
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Para hacer nuestra red neuronal decidimos darle la posibilidad de generar distintas estructuras para ella, nuestro mÃ©todo para hacer esto automatizadamente consiste guiarnos de una funciÃ³n (seleccionada por nosotros), para definir la cantidad de neuronas que queremos tener en el nÃºmero de capas que hayamos decidido poner.\n",
    "\n",
    "Si por ejemplo queremos una arquitectura que empieza con su primera capa con 20 neuronas, aumenta a 40 y sigue en 40 y despuÃ©s baja a 20 y llega hasta 3 podrÃ­amos guiarnos de esta funciÃ³n e ir mapeando ciertos puntos de ella:\n",
    "\n",
    "\n",
    "<iframe src=\"https://www.desmos.com/calculator/dy1goxwgu2?embed\" width=\"800\" height=\"400\" style=\"border: 1px solid #ccc\" frameborder=50></iframe> (hay un link con la grÃ¡fica, pero no sabemos porque Jupyter no la muestra. Igual en el .html se puede ver la grÃ¡fica por si acaso)\n",
    "\n",
    "\n",
    "En este caso estamos usando la funciÃ³n \n",
    "\n",
    "$$f\\left(x\\right)\\ =\\ 10\\cdot\\sqrt{x}\\ -\\ x\\ +\\ 20\\ \\left\\{0\\le x\\le130\\right\\}$$\n",
    "\n",
    "La cuÃ¡l fue definida a nuestro antojo para cumplir lo que querÃ­amos con nuestra red neuronal, que bÃ¡sicamente es que el nÃºmero de neuronas por capa aumente un poquito y luego caiga hasta llegar al nÃºmero de neuronas de salida. Con esta funciÃ³n definida lo que resta es evaluar el conjunto \n",
    "\n",
    "$$\\vec{x} = \\{ i*\\Delta X,\\quad i \\in \\mathbb{Z}^{*}, \\quad \\Delta X = \\frac{\\text{inicio dominio} - \\text{fin  dominio}}{\\text{num. capas}} \\}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "4d095fed47db45dab4a28f9ac7034b9e",
    "deepnote_app_coordinates": {
     "h": 29,
     "w": 12,
     "x": 0,
     "y": 382
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1681601818338,
    "source_hash": "ce280208"
   },
   "outputs": [],
   "source": [
    "def g1(domain: list)-> np.array:\n",
    "    \"\"\"\n",
    "    start at 20, grow and then decay function\n",
    "\n",
    "    INPUT:\n",
    "        - domain:   List of numeric values to be processed\n",
    "    OUTPUT: \n",
    "        - np.array: Array of integers resulting from the application of the mathematical formula\n",
    "    \"\"\"\n",
    "\n",
    "    return ((10*np.sqrt(domain)) - domain + 20).astype(np.int32)\n",
    "\n",
    "\n",
    "def g2(domain: list) -> np.array:\n",
    "    \"\"\"\n",
    "    f(x) = 20 -x, kind of function\n",
    "    \n",
    "    INPUT:\n",
    "        - domain:   list of numeric values to be processed\n",
    "    OUTPUT: \n",
    "        - np.array: array of integers resulting from the application of the mathematical formula\n",
    "    \"\"\"\n",
    "    \n",
    "    # apply the mathematical formula to each item in the list and convert the resulting array to integers\n",
    "    return (20 - (0.05 * np.array(domain) ** 3)).astype(np.int32)\n",
    "\n",
    "\n",
    "def g3(domain: list)-> np.array:\n",
    "    \"\"\"\n",
    "    Constant function.\n",
    "    \n",
    "    INPUT:\n",
    "        - domain:   list of numeric values (not used in this function)\n",
    "    OUTPUT: \n",
    "        - np.array: array of integers where each element is 20\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array([20 for i in range(len(domain))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ab406ebf62cb4263aa1c5770d93bd2ab",
    "deepnote_app_coordinates": {
     "h": 9,
     "w": 12,
     "x": 0,
     "y": 412
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Con nuestra abstracciÃ³n anterior, podemos proseguir a crear diferentes modelos simplemente cambiando la funciÃ³n del comportamiento del numero de neuronas. AsÃ­ definimos una funciÃ³n para construir diferentes modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "25580519473d40b9bcde50e5b5cd75a9",
    "deepnote_app_coordinates": {
     "h": 44,
     "w": 12,
     "x": 0,
     "y": 422
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1681601818338,
    "source_hash": "ce0cf2d6"
   },
   "outputs": [],
   "source": [
    "def build_different_models(input_sz: int,\n",
    "                           output_sz: int,\n",
    "                           max_layers: int, \n",
    "                           envelope_f: callable, \n",
    "                           lim: int) -> list:\n",
    "\n",
    "    \"\"\"\n",
    "    Builds a list of different neural network models with varying numbers of hidden layers and neurons.\n",
    "    \n",
    "    INPUT:\n",
    "        - input_sz:     integer specifying the size of the input layer.\n",
    "        - output_sz:    integer specifying the size of the output layer.\n",
    "        - max_layers:   integer specifying the maximum number of hidden layers allowed.\n",
    "        - envelope_f:   callable that takes in an array of sample points and returns an array of integers\n",
    "                        specifying the number of neurons in each hidden layer of a neural network model.\n",
    "        - lim:          integer specifying the maximum value of the sample points array, this is the \n",
    "                        limit of the domain of the function.\n",
    "    \n",
    "    OUTPUT: \n",
    "        - list of nn.ModuleList: list of different neural network models with varying architectures\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # list of lists, each list contains the number of neurons\n",
    "    # the model will have on each layer\n",
    "    hidden_layers_structure = []\n",
    "\n",
    "\n",
    "    # here we build the different neural architectures\n",
    "    for i in range(2, max_layers+1):\n",
    "        #delta = int((input_sz - output_sz)/i)\n",
    "        sample_points = np.linspace(0, lim, i)\n",
    "        structure     = envelope_f(sample_points) \n",
    "        hidden_layers_structure.append(structure)\n",
    "\n",
    "    # list of models\n",
    "    models = []\n",
    "\n",
    "    # populate the list of models\n",
    "    for structure in hidden_layers_structure:\n",
    "\n",
    "        # Define your layers for the model \n",
    "        model = nn.ModuleList()\n",
    "\n",
    "        # first layer\n",
    "        model.append(nn.Linear(input_sz, structure[0]))\n",
    "\n",
    "        # hidden layers\n",
    "        for i, n in enumerate(structure[:-1]):\n",
    "            model.append(nn.Linear(structure[i], structure[i+1]))\n",
    "\n",
    "        \n",
    "        # output layer\n",
    "        model.append(nn.Linear(structure[-1], output_sz))\n",
    "        model.append(nn.Linear(3, output_sz))\n",
    "\n",
    "        models.append(model)\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "96440bdd9ff64a5d9caca4e21f67c725",
    "deepnote_app_coordinates": {
     "h": 7,
     "w": 12,
     "x": 0,
     "y": 527
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Ahora continuamos definiendo nuestra _**clase NeuralNetwork**_ con la estructura de la red neuronal (haciendo uso de capas lineales y las diferentes funciones de activaciÃ³n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "70de3fffba704b3ba0f465366a36e753",
    "deepnote_app_coordinates": {
     "h": 59,
     "w": 12,
     "x": 0,
     "y": 467
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 100,
    "execution_start": 1681601818339,
    "source_hash": "fcb2672e"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "        A class that defines a neural network with a configurable number of layers\n",
    "        and activation function.\n",
    "\n",
    "        Attributes:\n",
    "            - flatten (nn.Flatten):   Flattens the input to a 1D tensor.\n",
    "            - model (nn.ModuleList):  Contains the layers of the neural network.\n",
    "            - output (nn.Softmax):    Applies softmax function to the output of the last layer.\n",
    "            - activation (nn.Module): Activation function used in the hidden layers.\n",
    "\n",
    "        Methods:\n",
    "            - __init__():             Initializes the NeuralNetwork class.\n",
    "            - forward(x):             Forward pass through the neural network.\n",
    "            - set_model(mod):         Sets the model layers.\n",
    "            - set_activation(act):    Sets the activation function used in the hidden layers.\n",
    "            - make_prediction(x):     Generates predictions for a given input.\n",
    "\n",
    "        Example:\n",
    "            >>> model = NeuralNetwork()\n",
    "            >>> model.set_activation(nn.LeakyReLU())\n",
    "            >>> model.set_model([nn.Linear(10, 20), nn.Linear(20, 2)])\n",
    "            >>> predictions = model.make_prediction(torch.randn(5, 10).numpy())\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.model   = nn.ModuleList()\n",
    "\n",
    "        self.output     = nn.Softmax()\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the neural network.\n",
    "\n",
    "        Args:\n",
    "            - x (torch.Tensor):      Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            - output (torch.Tensor): Output tensor of the neural network.\n",
    "        \"\"\"\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        for layer in self.model:\n",
    "            x = self.activation(layer(x))\n",
    "\n",
    "        return self.output(x)\n",
    "    \n",
    "    def set_model(self, mod):\n",
    "        \"\"\"\n",
    "        Sets the model layers.\n",
    "\n",
    "        Args:\n",
    "            - mod (list): List containing the neural network layers.\n",
    "        \"\"\"\n",
    "        self.model = mod\n",
    "\n",
    "    def set_activation(self, act: torch.nn.modules.activation):\n",
    "        \"\"\"\n",
    "        Sets the activation function used in the hidden layers.\n",
    "\n",
    "        Args:\n",
    "            - act (nn.Module): Activation function.\n",
    "        \"\"\"\n",
    "        self.activation = act\n",
    "\n",
    "    def make_prediction(self, x: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Generates predictions for a given input.\n",
    "\n",
    "        Args:\n",
    "            - x (np.array):     Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            - preds (np.array): Predictions tensor of the neural network.\n",
    "        \"\"\"\n",
    "        preds = self.forward(x).argmax(dim = 1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6bf6e58905244e899c3d331b983c442b",
    "deepnote_app_coordinates": {
     "h": 3,
     "w": 12,
     "x": 0,
     "y": 535
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Imprimimos nuestros posibles candidatos a modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "f1895027b44a4174ace152281a3decf5",
    "deepnote_app_coordinates": {
     "h": 30,
     "w": 12,
     "x": 0,
     "y": 539
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 61,
    "execution_start": 1681601818379,
    "source_hash": "d8e62acc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModuleList(\n",
       "   (0): Linear(in_features=20, out_features=20, bias=True)\n",
       "   (1): Linear(in_features=20, out_features=4, bias=True)\n",
       "   (2): Linear(in_features=4, out_features=3, bias=True)\n",
       "   (3): Linear(in_features=3, out_features=3, bias=True)\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): Linear(in_features=20, out_features=20, bias=True)\n",
       "   (1): Linear(in_features=20, out_features=35, bias=True)\n",
       "   (2): Linear(in_features=35, out_features=4, bias=True)\n",
       "   (3): Linear(in_features=4, out_features=3, bias=True)\n",
       "   (4): Linear(in_features=3, out_features=3, bias=True)\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): Linear(in_features=20, out_features=20, bias=True)\n",
       "   (1): Linear(in_features=20, out_features=42, bias=True)\n",
       "   (2): Linear(in_features=42, out_features=26, bias=True)\n",
       "   (3): Linear(in_features=26, out_features=4, bias=True)\n",
       "   (4): Linear(in_features=4, out_features=3, bias=True)\n",
       "   (5): Linear(in_features=3, out_features=3, bias=True)\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): Linear(in_features=20, out_features=20, bias=True)\n",
       "   (1): Linear(in_features=20, out_features=44, bias=True)\n",
       "   (2): Linear(in_features=44, out_features=35, bias=True)\n",
       "   (3): Linear(in_features=35, out_features=21, bias=True)\n",
       "   (4): Linear(in_features=21, out_features=4, bias=True)\n",
       "   (5): Linear(in_features=4, out_features=3, bias=True)\n",
       "   (6): Linear(in_features=3, out_features=3, bias=True)\n",
       " )]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_models = build_different_models(input_sz   = 20,\n",
    "                                          output_sz  = 3,\n",
    "                                          max_layers = 5, \n",
    "                                          envelope_f = g1, \n",
    "                                          lim = 130)\n",
    "candidate_models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1373921b5ac64d53bc1de1afa59e1d97",
    "deepnote_app_coordinates": {
     "h": 3,
     "w": 12,
     "x": 0,
     "y": 570
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Revisamos que estemos usando GPU y definimos el dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "bd2aa6faac274eef96148391d781d558",
    "deepnote_app_coordinates": {
     "h": 8,
     "w": 12,
     "x": 0,
     "y": 574
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 22,
    "execution_start": 1681601818419,
    "source_hash": "5600cde9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the GPU available? False\n",
      "Device cpu\n"
     ]
    }
   ],
   "source": [
    "gpu_avail = torch.cuda.is_available()\n",
    "print(f\"Is the GPU available? {gpu_avail}\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "46f40a283f014c4f9ddbd97e204de852",
    "deepnote_app_coordinates": {
     "h": 3,
     "w": 12,
     "x": 0,
     "y": 583
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Luego definimos el modelo, el optimizador y la funciÃ³n de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_id": "4f8c9006db474248b1fde6fd569dad8b",
    "deepnote_app_coordinates": {
     "h": 8,
     "w": 12,
     "x": 0,
     "y": 587
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1681601818479,
    "source_hash": "4696ac4f"
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "model.set_model(candidate_models[0])\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "837f1ed7ce9b4676ba8b0a89c69a58f0",
    "deepnote_app_coordinates": {
     "h": 11,
     "w": 12,
     "x": 0,
     "y": 596
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Entrenando la red y analizando el mejo modelo\n",
    "\n",
    "Luego de haber preparado los datos, definir la arquitectura, y de asÃ­ entrenar la red, vamos a definir la _**funciÃ³n de entrenamiento**_ para guardar nuestro mejor modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_id": "2549c548da674a0299e0a4ef887a9d65",
    "deepnote_app_coordinates": {
     "h": 63,
     "w": 12,
     "x": 0,
     "y": 608
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 48,
    "execution_start": 1681601818479,
    "source_hash": "3cad2daf"
   },
   "outputs": [],
   "source": [
    "# pass the model to the GPU device\n",
    "model.to(device)\n",
    "\n",
    "def train_model(model,\n",
    "                optimizer,\n",
    "                loss_module,\n",
    "                train_loader,\n",
    "                valid_loader, \n",
    "                num_epochs):\n",
    "    \"\"\"\n",
    "    Trains the specified model using the given data and hyperparameters, and saves the model with \n",
    "    the smallest validation error.\n",
    "\n",
    "    INPUT:\n",
    "        - model:        the PyTorch model to be trained\n",
    "        - optimizer:    the PyTorch optimizer used to update the model's parameters\n",
    "        - loss_module:  the PyTorch loss function used to calculate the training loss\n",
    "        - train_loader: the PyTorch DataLoader containing the training data\n",
    "        - valid_loader: the PyTorch DataLoader containing the validation data\n",
    "        - num_epochs:   the number of epochs to train the model for\n",
    "\n",
    "    OUTPUT:\n",
    "        None\n",
    "    \"\"\"\n",
    "  \n",
    "    # Let's find the smallest validation error value.\n",
    "    # So initialize it to 'infinity'\n",
    "    valid_loss_min = np.inf  \n",
    "  \n",
    "    for i in range(num_epochs):\n",
    "      \n",
    "        # We put the model in training mode.        \n",
    "        # It is important in other architectures such as convolutional networks.\n",
    "        model.train()  \n",
    "        train_loss = 0.0\n",
    "        v_loss = 0.0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            # move the attribute and label tensors to the GPU device\n",
    "            inputs, labels = data.to(device), target.to(device)\n",
    "            \n",
    "            # reset the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: calculate the output for the input data.\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = loss_module(outputs, labels)\n",
    "\n",
    "            # backpropagation: gradient calculation\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # update cost account across batches\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        train_loss = train_loss/len(train_loader.dataset) \n",
    "\n",
    "        # We put the model in evaluation mode.\n",
    "        model.eval() \n",
    "\n",
    "        # we are going to evaluate the trained model, calculating predictions with the validation set\n",
    "        for data,target in valid_loader:\n",
    "            data=data.to(device)\n",
    "            target=target.to(device)\n",
    "            output=model(data)\n",
    "            valid_loss= loss_module(output, target)\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    \n",
    "        # print training and validation statistics\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            i, train_loss, valid_loss))\n",
    "        \n",
    "\n",
    "        # We save the model with the smallest validation error.\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), 'model_bikeshare.pt')\n",
    "            valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ce2ac1b0490e41d590a8552ed430a89e",
    "deepnote_app_coordinates": {
     "h": 3,
     "w": 12,
     "x": 0,
     "y": 672
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Imprimimos los parÃ¡metros de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_id": "4a9e2dbea2104287ba373d8dff218b86",
    "deepnote_app_coordinates": {
     "h": 30,
     "w": 12,
     "x": 0,
     "y": 676
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6090,
    "execution_start": 1681601838239,
    "source_hash": "fbe0d70e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1735/2872644584.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output(x)\n",
      "Epoch: 0 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (inf --> 0.137084).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.102183 \tValidation Loss: 0.137084\n",
      "Validation loss decreased (0.137084 --> 0.137084).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.set_model(candidate_models[3])\n",
    "optimizer.zero_grad()\n",
    "\n",
    "train_model(model, optimizer, criterion, train_data, val_data, 50)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bb26c5d8bc5d4c67804d05e7b9dcc2d1",
    "deepnote_app_coordinates": {
     "h": 4,
     "w": 12,
     "x": 0,
     "y": 707
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Y finalmente, evaluamos el desempeÃ±o del modelo que creemos que es mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_id": "b8493c18318445c1912700fc6014e7aa",
    "deepnote_app_coordinates": {
     "h": 22,
     "w": 12,
     "x": 0,
     "y": 712
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 33,
    "execution_start": 1681601844297,
    "source_hash": "4796fe0d"
   },
   "outputs": [],
   "source": [
    "def test_model_performance(model: NeuralNetwork, test_set: Dataset): \n",
    "    \"\"\"\n",
    "    Evaluates the performance of a neural network model on a given test dataset.\n",
    "\n",
    "    INPUT:\n",
    "        - model:    A neural network model to be tested.\n",
    "        - test_set: A dataset containing test samples and corresponding labels.\n",
    "\n",
    "    OUTPUT:\n",
    "        - accuracy: The accuracy of the model on the test dataset, expressed as a percentage.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # we feed the model with the data in order to get the outputs\n",
    "    # and with all the outputs predicted by de model, we proceed\n",
    "    # to get the maximum of each prediction category\n",
    "    preds = model(test_set.X).argmax(dim = 1)\n",
    "\n",
    "    # now we compare with the expected outcomes\n",
    "    target = test_set.y.argmax(dim = 1)\n",
    "\n",
    "    # we compare both and get the total number of correct answers given \n",
    "    # by the model\n",
    "    correct = np.sum( np.array((preds == target).flatten()) ).astype('int32')\n",
    "    accuracy = correct/len(preds) \n",
    "\n",
    "    return accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_id": "cdb29fdcd0734e7ba7950a3bce29ab61",
    "deepnote_app_coordinates": {
     "h": 8,
     "w": 12,
     "x": 0,
     "y": 735
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 22,
    "execution_start": 1681601844318,
    "source_hash": "ede0edf2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1735/2872644584.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39.325842696629216"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_performance(model, test_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "aafcc4ed2b754aa9afa8aed8adf2e6a0",
    "deepnote_app_coordinates": {
     "h": 14,
     "w": 12,
     "x": 0,
     "y": 744
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Mejoras para el modelo\n",
    "\n",
    "Como se puede observar anteriormente, no tenemos un buen accuracy como estamos definiendo nuestro modelo, por lo anterior decidimos crear una funciÃ³n que nos ayudara a ver cuÃ¡l modelo serÃ­a el mejor, para esto, la funciÃ³n nos permite crear diferentes arquitecturas de la red neuronal, y tambiÃ©n utilizar varias funciones de activaciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_id": "cd92cbd427f7458882e665119ec0a2a0",
    "deepnote_app_coordinates": {
     "h": 61,
     "w": 12,
     "x": 0,
     "y": 759
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1681601844667,
    "source_hash": "6d6695fc"
   },
   "outputs": [],
   "source": [
    "def try_everything(input_sz:   int,\n",
    "                   output_sz:   int,\n",
    "                   funs:       list,\n",
    "                   lims:       list,\n",
    "                   acts:       list,\n",
    "                   max_layers: int,\n",
    "                   n_epochs:   int, \n",
    "                   criterion:  torch.nn.modules.loss, \n",
    "                   data:       tuple):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        - input_sz:   the size of the input layer\n",
    "        - output_sz:  the size of the output layer\n",
    "        - funs:       a list of functions to be used to generate the hidden layer structures\n",
    "        - lims:       a list of the upper limits of the range of the input to each function in funs\n",
    "        - acts:       a list of activation functions to be used\n",
    "        - max_layers: the maximum number of layers allowed in the neural network\n",
    "        - n_epochs:   the number of epochs to train the neural network for\n",
    "        - criterion:  the loss function to be used\n",
    "        - data:       a tuple of three datasets, consisting of training data,\n",
    "                      validation data, and test data\n",
    "        \n",
    "    OUTPUT:\n",
    "        - a tuple of the best score achieved by any of the models tested and the model that achieved \n",
    "          that score\n",
    "    \"\"\"\n",
    "\n",
    "    train_sec, val_sec, test_sec = data\n",
    "\n",
    "    # list of models\n",
    "    models = []\n",
    "\n",
    "    # list of each model score\n",
    "    scores = []  \n",
    "\n",
    "    for act in acts:\n",
    "        for fun, lim in zip(funs, lims):\n",
    "            for i in range(2, max_layers+1):\n",
    "\n",
    "                sample_points = np.linspace(0, lim, i)\n",
    "                structure     = fun(sample_points) \n",
    "\n",
    "                model = NeuralNetwork()\n",
    "                \n",
    "                # Define your layers for the model \n",
    "                model_struc = nn.ModuleList()\n",
    "\n",
    "                # first layer\n",
    "                model_struc.append(nn.Linear(input_sz, structure[0]))\n",
    "\n",
    "                # hidden layers\n",
    "                for i, n in enumerate(structure[:-1]):\n",
    "                    model_struc.append(nn.Linear(structure[i], structure[i+1]))\n",
    "        \n",
    "                # output layer\n",
    "                model_struc.append(nn.Linear(structure[-1], output_sz))\n",
    "                model_struc.append(nn.Linear(3, output_sz))\n",
    "\n",
    "                #store the structure into the model \n",
    "                model.set_model(model_struc)\n",
    "                model.set_activation(act)\n",
    "\n",
    "                #store the model\n",
    "                models.append(model)\n",
    "                \n",
    "    score = -np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for model in models:\n",
    "        optimizer.zero_grad()\n",
    "        train_model(model,\n",
    "                    optimizer,\n",
    "                    criterion,\n",
    "                    train_data,\n",
    "                    val_data,\n",
    "                    n_epochs) \n",
    "\n",
    "        evaluation = test_model_performance(model, test_sec)\n",
    "\n",
    "        if (score < evaluation):\n",
    "            best_model = model\n",
    "            score = evaluation\n",
    "    \n",
    "    return (score, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6daa7d494717408796ec3bc3609ace52",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 821
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Probamos la funciÃ³n, con dos arquitecturas de redes diferentes y cuatro funciones de activaciones diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_id": "efac7390726440bf919203639f5424bf",
    "deepnote_app_coordinates": {
     "h": 35,
     "w": 12,
     "x": 0,
     "y": 827
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 264414,
    "execution_start": 1681601846372,
    "source_hash": "a92f01d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.127648).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.105996 \tValidation Loss: 0.127648\n",
      "Validation loss decreased (0.127648 --> 0.127648).  Saving model ...\n",
      "Epoch: 0 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (inf --> 0.140249).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.100843 \tValidation Loss: 0.140249\n",
      "Validation loss decreased (0.140249 --> 0.140249).  Saving model ...\n",
      "Epoch: 0 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (inf --> 0.136674).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.101065 \tValidation Loss: 0.136674\n",
      "Validation loss decreased (0.136674 --> 0.136674).  Saving model ...\n",
      "Epoch: 0 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (inf --> 0.137211).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.098743 \tValidation Loss: 0.137211\n",
      "Validation loss decreased (0.137211 --> 0.137211).  Saving model ...\n",
      "Epoch: 0 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (inf --> 0.129669).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.098863 \tValidation Loss: 0.129669\n",
      "Validation loss decreased (0.129669 --> 0.129669).  Saving model ...\n",
      "Epoch: 0 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (inf --> 0.134417).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.101346 \tValidation Loss: 0.134417\n",
      "Validation loss decreased (0.134417 --> 0.134417).  Saving model ...\n",
      "Epoch: 0 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (inf --> 0.133367).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.101369 \tValidation Loss: 0.133367\n",
      "Validation loss decreased (0.133367 --> 0.133367).  Saving model ...\n",
      "Epoch: 0 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (inf --> 0.137088).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.096408 \tValidation Loss: 0.137088\n",
      "Validation loss decreased (0.137088 --> 0.137088).  Saving model ...\n",
      "Epoch: 0 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (inf --> 0.139328).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.105676 \tValidation Loss: 0.139328\n",
      "Validation loss decreased (0.139328 --> 0.139328).  Saving model ...\n",
      "Epoch: 0 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (inf --> 0.131371).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.101520 \tValidation Loss: 0.131371\n",
      "Validation loss decreased (0.131371 --> 0.131371).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "score, best_model = try_everything(input_sz = 20,\n",
    "                                   output_sz = 3,\n",
    "                                   funs = [g1, g2, g3],\n",
    "                                   lims = [130, 6, 1],\n",
    "                                   acts = [nn.ReLU(), nn.LeakyReLU(), nn.Sigmoid(), nn.Tanh()],\n",
    "                                   max_layers = 5,\n",
    "                                   n_epochs   = 50, \n",
    "                                   criterion  = nn.CrossEntropyLoss(), \n",
    "                                   data       = (train_sec, val_sec, test_sec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "286b6f4bcf5a4fcdb8dbf594fdc018a0",
    "deepnote_app_coordinates": {
     "h": 6,
     "w": 12,
     "x": 0,
     "y": 879
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Como se puede observar el resultado no es que mejore mucho, pero aumenta algo a diferencia de nuestro primer modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_id": "3fdd2d7aee4e4195beadfcd22d037092",
    "deepnote_app_coordinates": {
     "h": 15,
     "w": 12,
     "x": 0,
     "y": 863
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 23,
    "execution_start": 1681602110791,
    "source_hash": "1f8baaa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuestro modelo ganador tuvo 46.06741573033708% de precisiÃ³n y fue:\n",
      "\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (model): ModuleList(\n",
      "    (0): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (2): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (3): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (4): Linear(in_features=3, out_features=3, bias=True)\n",
      "  )\n",
      "  (output): Softmax(dim=None)\n",
      "  (activation): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nuestro modelo ganador tuvo {score}% de precisiÃ³n y fue:\\n\")\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a14e40ce214b4744aa2c5b9d94218aeb",
    "deepnote_app_coordinates": {
     "h": 20,
     "w": 12,
     "x": 0,
     "y": 886
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "AdemÃ¡s, al elegir de manera aleatoria un registro del dataset y utilizar el modelo para predecir su sentimiento, en la mayorÃ­a de casos nos estÃ¡ dando la emociÃ³n enojado, no entendemos por quÃ© el modelo se cesga a tratar de clasificar los audios como bravos. Nuestra hipÃ³tesis es que faltan mÃ¡s datos para que el modelo pueda entender mejor.\n",
    "\n",
    "Decimos esto en vista de haber probado todas las opciones que se nos ocurrieron; probamos distintas funciones de activaciÃ³n, nÃºmero distinto de capas, distintas arquitecturas de red neuronal, datos estandarizados y sin estandarizar, sin embargo no fue suficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cell_id": "aabfd8a24eb841ba9866dcc2343c6930",
    "deepnote_app_coordinates": {
     "h": 13,
     "w": 12,
     "x": 0,
     "y": 907
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1681602110821,
    "source_hash": "532837ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1870,  0.5220, -0.5535, -1.5060, -1.2164,  1.2165, -1.2264, -1.0250,\n",
      "          1.1427,  0.3959, -0.1975, -1.1870, -2.0505, -0.5055, -1.0895, -0.6271,\n",
      "         -0.0472, -0.6428, -0.6431, -0.7652]])\n",
      "/tmp/ipykernel_1735/2872644584.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.output(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randint(test_sec.X.shape[0])\n",
    "sample = test_sec.X[a,:]\n",
    "\n",
    "print(sample.reshape((1,-1)))\n",
    "best_model.make_prediction(sample.reshape((1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "bf9c51906f404b36906aed20e4545cc6",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=839e86b6-ffa4-40c1-9d7e-74e8c82a69a5' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_app_layout": "article",
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "ecaa1bfc5b3240848cf819ea82fdf830",
  "deepnote_persisted_session": {
   "createdAt": "2023-04-16T00:01:11.611Z"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
